{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7ec02f-bc61-4c89-987d-b3025b2a474f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from rdkit import Chem\n",
    "from sklearn.metrics import mean_absolute_error, root_mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch_geometric as tg\n",
    "from torch_geometric.data import Dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import MessagePassing, global_add_pool\n",
    "\n",
    "import optuna\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f577692",
   "metadata": {},
   "source": [
    "# Datasets and Dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22e0b7a",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb419790-ae4f-478b-8ecf-843e1aa4375a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding_unk(value, choices: list) -> list:\n",
    "    # One hot encoding with unknown value handling\n",
    "    # If the value is in choices, it puts a 1 at the corresponding index\n",
    "    # Otherwise, it puts a 1 at the last index (unknown)\n",
    "    encoding = [0] * (len(choices) + 1)\n",
    "    index = choices.index(value) if value in choices else -1\n",
    "    encoding[index] = 1\n",
    "    return encoding\n",
    "\n",
    "\n",
    "def get_atom_features(atom) -> list:\n",
    "    # Returns a feature list for the atom\n",
    "    # Concatenates the one-hot encodings into a single list\n",
    "    atom_features = [\n",
    "        one_hot_encoding_unk(atom.GetSymbol(), ['B','Be','Br','C','Cl','F','I','N','Nb','O','P','S','Se','Si','V','W']),\n",
    "        one_hot_encoding_unk(atom.GetTotalDegree(), [0, 1, 2, 3, 4, 5]),\n",
    "        one_hot_encoding_unk(atom.GetFormalCharge(), [-1, -2, 1, 2, 0]),\n",
    "        one_hot_encoding_unk(int(atom.GetTotalNumHs()), [0, 1, 2, 3, 4]),\n",
    "        one_hot_encoding_unk(int(atom.GetHybridization()),[\n",
    "                                                        Chem.rdchem.HybridizationType.SP,\n",
    "                                                        Chem.rdchem.HybridizationType.SP2,\n",
    "                                                        Chem.rdchem.HybridizationType.SP3,\n",
    "                                                        Chem.rdchem.HybridizationType.SP3D,\n",
    "                                                        Chem.rdchem.HybridizationType.SP3D2\n",
    "                                                        ]),\n",
    "        [1 if atom.GetIsAromatic() else 0],\n",
    "        [atom.GetMass() * 0.01]\n",
    "    ]\n",
    "    return sum(atom_features, []) # Flatten the list into a single list\n",
    "\n",
    "\n",
    "def get_bond_features(bond) -> list:\n",
    "    # Returns a one-hot encoded feature list for the bond\n",
    "    bond_fdim = 7\n",
    "\n",
    "    if bond is None:\n",
    "        bond_features = [1] + [0] * (bond_fdim - 1)\n",
    "    else:\n",
    "        bt = bond.GetBondType()\n",
    "        bond_features = [\n",
    "            0,  # Zeroth index indicates if bond is None\n",
    "            bt == Chem.rdchem.BondType.SINGLE,\n",
    "            bt == Chem.rdchem.BondType.DOUBLE,\n",
    "            bt == Chem.rdchem.BondType.TRIPLE,\n",
    "            bt == Chem.rdchem.BondType.AROMATIC,\n",
    "            (bond.GetIsConjugated() if bt is not None else 0),\n",
    "            (bond.IsInRing() if bt is not None else 0)\n",
    "        ]\n",
    "    return bond_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bd5653",
   "metadata": {},
   "source": [
    "### Dataset Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af251a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MolGraph:\n",
    "    # Returns a custom molecular graph for a given SMILES string\n",
    "    # Contains atom, bond features and node connectivity\n",
    "    def __init__(self, smiles: str):\n",
    "        self.smiles = smiles\n",
    "        self.atom_features = []\n",
    "        self.bond_features = []\n",
    "        self.edge_index = []\n",
    "\n",
    "        molecule = Chem.MolFromSmiles(self.smiles)\n",
    "        n_atoms = molecule.GetNumAtoms()\n",
    "\n",
    "        for atom_1 in range(n_atoms):\n",
    "            self.atom_features.append(get_atom_features(molecule.GetAtomWithIdx(atom_1)))\n",
    "\n",
    "            for atom_2 in range(atom_1 + 1, n_atoms):\n",
    "                bond = molecule.GetBondBetweenAtoms(atom_1, atom_2)\n",
    "                if bond is None:\n",
    "                    continue\n",
    "                bond_features = get_bond_features(bond)\n",
    "                self.bond_features.append(bond_features)\n",
    "                self.bond_features.append(bond_features) # Bond features are added twice for both directions\n",
    "                self.edge_index.extend([(atom_1, atom_2), (atom_2, atom_1)]) # Edge index list with tuples of connected nodes instead of adjacency matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6313a147",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChemDataset(Dataset):\n",
    "    def __init__(self, smiles: str, labels, flip_prob: float=0.5, noise_std: float=0.5, precompute: bool=True):\n",
    "        # Choose here how much noise to add for the denoising task\n",
    "        super(ChemDataset, self).__init__()\n",
    "        self.smiles = smiles\n",
    "        self.labels = labels\n",
    "        self.cache = {}\n",
    "        self.flip_prob = flip_prob\n",
    "        self.noise_std = noise_std\n",
    "        self.precompute = precompute\n",
    "\n",
    "        # Precomputing the dataset so the get method is faster, and the GPU doesn't have to wait for the CPU\n",
    "        if precompute:\n",
    "            print(f\"Precomputing data...\")\n",
    "            with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "                futures = [\n",
    "                    executor.submit(self.process_key , idx)\n",
    "                    for idx in range(len(self.smiles))\n",
    "                ]\n",
    "\n",
    "                for future in as_completed(futures):\n",
    "                    future.result()\n",
    "\n",
    "            print(f\"Precomputation finished. {len(self.cache)} molecules cached.\")\n",
    "\n",
    "    def process_key(self, key):\n",
    "        # Process the key to get the corresponding molecule graph\n",
    "        # If the molecule is already cached, return it\n",
    "        smiles = self.smiles[key]\n",
    "        if smiles in self.cache.keys():\n",
    "            molecule = self.cache[smiles]\n",
    "        else:\n",
    "            molgraph = MolGraph(smiles)\n",
    "            molecule = self.molgraph2data(molgraph, key)\n",
    "            self.cache[smiles] = molecule\n",
    "        return molecule\n",
    "\n",
    "    def molgraph2data(self, molgraph, key):\n",
    "        data = tg.data.Data()\n",
    "\n",
    "        # Coverting all features and labels to tensors\n",
    "        # And adding it to the data object\n",
    "        data.x = torch.tensor(molgraph.atom_features, dtype=torch.float)\n",
    "        data.edge_index = torch.tensor(molgraph.edge_index, dtype=torch.long).t().contiguous()\n",
    "        data.edge_attr = torch.tensor(molgraph.bond_features, dtype=torch.float)\n",
    "        data.y = torch.tensor([self.labels[key]], dtype=torch.float)\n",
    "        data.smiles = self.smiles[key]\n",
    "\n",
    "        if self.flip_prob > 0 or self.noise_std > 0:\n",
    "            # Create a deep copy to avoid modifying original data\n",
    "            x_noisy = deepcopy(data.x)\n",
    "            edge_attr_noisy = deepcopy(data.edge_attr)\n",
    "            \n",
    "            # Apply bit flipping to binary features if probability > 0\n",
    "            if self.flip_prob > 0:\n",
    "                binary_features = x_noisy[:, :-1]  # All but last column, which contains mass\n",
    "                flip_mask = torch.rand_like(binary_features) < self.flip_prob\n",
    "                binary_features[flip_mask] = 1.0 - binary_features[flip_mask]  # Flip 0->1 and 1->0\n",
    "                x_noisy[:, :-1] = deepcopy(binary_features)\n",
    "\n",
    "                binary_features = edge_attr_noisy # Edge features only contain one-hot encodings\n",
    "                flip_mask = torch.rand_like(binary_features) < self.flip_prob\n",
    "                binary_features[flip_mask] = 1.0 - binary_features[flip_mask]  # Flip 0->1 and 1->0\n",
    "                edge_attr_noisy = deepcopy(binary_features)\n",
    "            \n",
    "            # Apply Gaussian noise to continuous feature if std > 0\n",
    "            if self.noise_std > 0:\n",
    "                mass_feature = x_noisy[:, -1:]  # Just the last column, which contains mass\n",
    "                # Adding noise which is a percentage of the mass feature\n",
    "                mass_feature += mass_feature * torch.randn_like(mass_feature) * self.noise_std\n",
    "                x_noisy[:, -1:] = deepcopy(mass_feature)\n",
    "            \n",
    "            data.x_noisy = x_noisy\n",
    "            data.edge_attr_noisy = edge_attr_noisy\n",
    "\n",
    "        return data\n",
    "\n",
    "    def get(self,key):\n",
    "        return self.process_key(key)\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        # Standard get method for PyTorch Dataset\n",
    "        return self.process_key(key)\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.smiles)\n",
    "\n",
    "    def __len__(self):\n",
    "        # Standard len method for PyTorch Dataset\n",
    "        return len(self.smiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599a85cc",
   "metadata": {},
   "source": [
    "### Dataloader construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65270799",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_loader(data_df: pd.DataFrame, smiles_column: str, target_column: str, shuffle: bool=True, batch_size: int=16):  \n",
    "    # Constructs a PyTorch Geometric DataLoader from a DataFrame\n",
    "    # Takes the SMILES and target column names as input\n",
    "    assert len(data_df) > 0, \"DataFrame is empty\"\n",
    "      \n",
    "    smiles = data_df[smiles_column].values\n",
    "    labels = data_df[target_column].values.astype(np.float32)  \n",
    "    \n",
    "    dataset = ChemDataset(smiles, labels)\n",
    "    loader = DataLoader(dataset=dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=shuffle,\n",
    "                            pin_memory=True\n",
    "                       )\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35730aa3",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84630b7b",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfaef5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DMPNNConv(MessagePassing): \n",
    "    # Extending the MessagePassing class from PyG\n",
    "    # Used for the convolutional layers in the encoder\n",
    "    def __init__(self, hidden_size: int):\n",
    "        super(DMPNNConv, self).__init__(aggr='add') # Sum aggregation function, most expressive aggregation as far as I know\n",
    "        self.linear = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, edge_index, edge_attr):\n",
    "        row, _ = edge_index\n",
    "        # Since each edge is bidirectional, we do two message passings, one for each direction\n",
    "        aggregated_message = self.propagate(edge_index, x=None, edge_attr=edge_attr)\n",
    "        reversed_message = torch.flip(edge_attr.view(edge_attr.size(0) // 2, 2, -1), dims=[1]).view(edge_attr.size(0), -1)\n",
    "\n",
    "        return aggregated_message, self.linear(aggregated_message[row] - reversed_message)\n",
    "\n",
    "    def message(self, edge_attr):\n",
    "        return edge_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce58a11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNEncoder(nn.Module):\n",
    "    def __init__(self, num_node_features: int, num_edge_features: int, hidden_size: int, mode: str, depth: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.depth = depth\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout = dropout\n",
    "        self.mode = mode\n",
    "\n",
    "        # Encoder layers\n",
    "        self.edge_init = nn.Linear(num_node_features + num_edge_features, hidden_size)\n",
    "        self.convs = nn.ModuleList([DMPNNConv(hidden_size) for _ in range(depth)])\n",
    "        self.edge_to_node = nn.Linear(num_node_features + hidden_size, hidden_size)\n",
    "        self.pool = global_add_pool  # Not learnable\n",
    "\n",
    "    def forward(self, data):\n",
    "        edge_index, edge_attr, batch = data.edge_index, data.edge_attr, data.batch\n",
    "        \n",
    "        if self.mode == 'denoise':\n",
    "            x = data.x_noisy\n",
    "            edge_attr = data.edge_attr_noisy\n",
    "        elif self.mode == 'predict':\n",
    "            x = data.x\n",
    "        else:\n",
    "            raise ValueError(\"Invalid mode. Choose 'denoise' or 'predict'.\")\n",
    "\n",
    "        # Edge initialization\n",
    "        row, _ = edge_index\n",
    "        h_0 = F.relu(self.edge_init(torch.cat([x[row], edge_attr], dim=1)))\n",
    "        h = h_0\n",
    "\n",
    "        # DMPNN Conv layers\n",
    "        for layer in self.convs:\n",
    "            _, h = layer(edge_index, h)\n",
    "            h += h_0\n",
    "            h = F.dropout(F.relu(h), self.dropout, training=self.training)\n",
    "\n",
    "        # Edge to node aggregation\n",
    "        # Re-using the last layer's results for s\n",
    "        s, _ = self.convs[-1](edge_index, h)\n",
    "        \n",
    "        # Due to a recurring error which I can't figure out, we add a check here\n",
    "        # to ensure that the sizes of s and x match\n",
    "        # This is a workaround and should be fixed in the future\n",
    "        # Luckily, this issue only occurs for batches with batch size 1\n",
    "\n",
    "        # Pad/truncate s to match x's size\n",
    "        if s.shape[0] != x.shape[0]:\n",
    "            # Create tensor with same length as x (regardless of connectivity)\n",
    "            s_fixed = torch.zeros(x.shape[0], self.hidden_size, device=s.device)\n",
    "            # Only use the connected nodes we have (first min(s.shape[0], x.shape[0]))\n",
    "            min_len = min(s.shape[0], x.shape[0]) \n",
    "            s_fixed[:min_len] = s[:min_len]\n",
    "            s = s_fixed\n",
    "\n",
    "        q = torch.cat([x, s], dim=1)\n",
    "        h = F.relu(self.edge_to_node(q))\n",
    "\n",
    "        # Global pooling for the final node embeddings\n",
    "        embedding = self.pool(h, batch)\n",
    "        \n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f3f8dc",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12dcc5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNDecoder(nn.Module):\n",
    "    # Decoder for self-supervised denoising task\n",
    "    # Decoding both node and edge features\n",
    "    def __init__(self, hidden_size: int, num_node_features: int, num_edge_features: int, dropout: float):\n",
    "        super().__init__()\n",
    "        # Node decoding layer\n",
    "        self.node_lin = nn.Linear(hidden_size, num_node_features)\n",
    "        # Edge decoding layers\n",
    "        self.edge_lin = nn.Linear(hidden_size, num_edge_features)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, graph_embedding, batch, edge_index):\n",
    "        # Decode node features\n",
    "        batch_size = graph_embedding.size(0)\n",
    "        node_counts = torch.bincount(batch)  # number of nodes in each graph\n",
    "\n",
    "        # Expand each graph embedding for nodes\n",
    "        expanded_nodes = []\n",
    "        for g in range(batch_size):\n",
    "            expanded_nodes.append(graph_embedding[g].unsqueeze(0).repeat(node_counts[g], 1))\n",
    "\n",
    "        # Concatenate along node dimension\n",
    "        expanded_nodes = torch.cat(expanded_nodes, dim=0)  # total_nodes x hidden_size\n",
    "\n",
    "        # Decode node features\n",
    "        x_hat = F.dropout(expanded_nodes, p=self.dropout, training=self.training)\n",
    "        x_hat = self.node_lin(x_hat)\n",
    "        \n",
    "        # Decode edge features\n",
    "        # Map edges to their source node's graph\n",
    "        edge_src = edge_index[0]  # Source nodes of edges\n",
    "        edge_batch = batch[edge_src]  # Batch indices of source nodes (which graph they belong to)\n",
    "        \n",
    "        # Expand graph embeddings for edges\n",
    "        expanded_edges = graph_embedding[edge_batch]  # shape = (num_edges, hidden_size)\n",
    "        \n",
    "        # Decode edge features\n",
    "        edge_hat = F.dropout(expanded_edges, p=self.dropout, training=self.training)\n",
    "        edge_hat = self.edge_lin(edge_hat)\n",
    "\n",
    "        return x_hat, edge_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d53e923",
   "metadata": {},
   "source": [
    "### Prediction Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dba7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNHead(nn.Module):\n",
    "    # Prediction Head for prediction solubility\n",
    "    def __init__(self, hidden_size: int, dropout: float):\n",
    "        super().__init__()\n",
    "        # Only some FFN layers which get the embedding as input\n",
    "        self.ffn1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.ffn2 = nn.Linear(hidden_size, 1)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, graph_embedding):\n",
    "        x = F.relu(self.ffn1(graph_embedding))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        return self.ffn2(x).squeeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203f76fe",
   "metadata": {},
   "source": [
    "### Main Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6dc19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(nn.Module):\n",
    "    # The main GNN model which brings together the encoder, decoder and head\n",
    "    # It has two modes, denoise and predict\n",
    "    # The encoder branches out to the decoder and head\n",
    "    def __init__(self, num_node_features: int, num_edge_features: int, hidden_size: int=300, depth: int=5, mode: str='denoise', dropout: float=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder = GNNEncoder(num_node_features, num_edge_features, hidden_size=hidden_size, mode=mode, depth=depth, dropout=dropout)\n",
    "        self.head = GNNHead(hidden_size=hidden_size, dropout=dropout)\n",
    "        self.decoder = GNNDecoder(hidden_size=hidden_size, num_node_features=num_node_features, num_edge_features=num_edge_features, dropout=dropout)\n",
    "\n",
    "    def set_mode(self, mode: str):\n",
    "        # Update the mode in the encoder\n",
    "        # So the encoder knows if it needs to read noisy or noise-free data\n",
    "        self.encoder.mode = mode\n",
    "\n",
    "    def get_embedding(self, data):\n",
    "        # Get the graph embedding from the encoder\n",
    "        graph_embedding = self.encoder(data)\n",
    "        return graph_embedding\n",
    "\n",
    "    def forward(self, data):\n",
    "        graph_embedding = self.encoder(data)\n",
    "\n",
    "        if self.encoder.mode == 'predict':\n",
    "            prediction = self.head(graph_embedding)\n",
    "            return prediction\n",
    "        \n",
    "        elif self.encoder.mode == 'denoise':\n",
    "            node_features, edge_features = self.decoder(graph_embedding, data.batch, data.edge_index)\n",
    "            return node_features, edge_features\n",
    "    \n",
    "        else:\n",
    "            raise ValueError(\"Invalid mode. Choose 'predict' or 'denoise'.\")\n",
    "        \n",
    "    def encode(self, data):\n",
    "        return self.encoder(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd052185",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Standardizer:\n",
    "    # Standardizer for the solubility values\n",
    "    def __init__(self, mean: float, std: float):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, x, rev: bool=False):\n",
    "        if rev:\n",
    "            return (x * self.std) + self.mean\n",
    "        return (x - self.mean) / self.std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83402641",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, loss, alpha: float, stdzer: Standardizer=None):\n",
    "    # Train the model for one epoch on the denoising and prediction tasks simultaneously\n",
    "    model.train()\n",
    "    total_loss_count = 0\n",
    "    denoise_loss_count = 0\n",
    "    pred_loss_count = 0\n",
    "\n",
    "    # Unfreeze all parts of the model\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Get losses for the denoising task\n",
    "        model.set_mode('denoise')\n",
    "        node_out, edge_out = model(batch)\n",
    "        node_loss = loss(node_out, batch.x)\n",
    "        edge_loss = loss(edge_out, batch.edge_attr)\n",
    "        denoise_loss = alpha * (node_loss + edge_loss)\n",
    "\n",
    "        # Get losses for the prediction task\n",
    "        model.set_mode('predict')\n",
    "        pred_out = model(batch)\n",
    "        pred_loss = (1 - alpha) * loss(pred_out, stdzer(batch.y))\n",
    "\n",
    "        # Combine the weighted losses as a sum and backpropagate them\n",
    "        combined_loss = denoise_loss + pred_loss\n",
    "        combined_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Cui et al. did a non-weighted sum of self-supervised and supervised losses\n",
    "        # https://doi.org/10.1038/s41467-025-57101-4\n",
    "\n",
    "        # Wang et al. did a weighted sum of self-supervised and supervised losses\n",
    "        # https://doi.org/10.48550/arXiv.2210.08813\n",
    "\n",
    "        # Also experimented with alternating backpropagation steps for each task\n",
    "        # Results were worse\n",
    "\n",
    "        total_loss_count += combined_loss.item()\n",
    "        denoise_loss_count += denoise_loss.item()\n",
    "        pred_loss_count += pred_loss.item()\n",
    "\n",
    "    return math.sqrt(total_loss_count / len(loader.dataset)), math.sqrt(denoise_loss_count / len(loader.dataset)), math.sqrt(pred_loss_count / len(loader.dataset))\n",
    "\n",
    "\n",
    "def train_epoch_without_SSL(model, loader, optimizer, loss, alpha: float, stdzer: Standardizer=None):\n",
    "    # Train the model for one epoch on the prediction task only\n",
    "    # This is only needed for reference\n",
    "    # Unfreeze the encoder\n",
    "    for param in model.encoder.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    # Freeze the decoder\n",
    "    for param in model.decoder.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Unfreeze the prediction head\n",
    "    for param in model.head.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    # Train the model for one epoch, either on denoising or prediction task\n",
    "    model.train()\n",
    "    pred_loss_count = 0\n",
    "\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Get losses for the prediction task\n",
    "        model.set_mode('predict')\n",
    "        pred_out = model(batch)\n",
    "        pred_loss = (1 - alpha) * loss(pred_out, stdzer(batch.y))\n",
    "\n",
    "        # Only backpropagate the prediction loss\n",
    "        pred_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        pred_loss_count += pred_loss.item()\n",
    "\n",
    "    return math.sqrt(pred_loss_count / len(loader.dataset))\n",
    "\n",
    "\n",
    "def pred(model, loader, mode: str, stdzer: Standardizer=None):\n",
    "    # Predict with the model, either on denoising or prediction task\n",
    "    # No test-time adaptation here, just a simple forward pass\n",
    "    if mode == 'denoise':\n",
    "        model.set_mode('denoise')\n",
    "        model.eval()\n",
    "\n",
    "        preds = []\n",
    "        with torch.no_grad():\n",
    "            for batch in loader:\n",
    "                batch = batch.to(device)\n",
    "                node_out, edge_out = model(batch)\n",
    "                node_out.cpu().detach().flatten().tolist()\n",
    "                preds.extend(node_out.cpu().detach().flatten().tolist() + edge_out.cpu().detach().flatten().tolist())\n",
    "                \n",
    "        return preds\n",
    "\n",
    "    elif mode == 'predict':\n",
    "        model.set_mode('predict')\n",
    "        model.eval()\n",
    "\n",
    "        preds = []\n",
    "        with torch.no_grad():\n",
    "            for batch in loader:\n",
    "                batch = batch.to(device)\n",
    "                out = model(batch)\n",
    "                pred = stdzer(out, rev=True)\n",
    "                preds.extend(pred.cpu().detach().tolist())\n",
    "\n",
    "        return preds\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Invalid mode. Choose 'denoise' or 'predict'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089eeedd",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0998c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f33b3f",
   "metadata": {},
   "source": [
    "### Loading and preprocessing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e699e0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "data_df = pd.read_csv(\"data/AqSolDBc.csv\")\n",
    "# Drop single atoms    \n",
    "idx_single = [i for i,s in enumerate(data_df['SmilesCurated']) if Chem.MolFromSmiles(s).GetNumAtoms() == 1 or '.' in s]\n",
    "data_df = data_df.drop(idx_single)\n",
    "if len(idx_single) > 0:\n",
    "    print(f\"Removing {idx_single} due to single atoms\")\n",
    "\n",
    "test_df = pd.read_csv(\"data/OChemUnseen.csv\")\n",
    "# Drop some Nonetypes\n",
    "idx_nonetype = [i for i,s in enumerate(test_df['SMILES']) if Chem.MolFromSmiles(s) is None] # Got an error for a SMILES which was None\n",
    "test_df = test_df.drop(idx_nonetype)\n",
    "if len(idx_nonetype) > 0:\n",
    "    print(f\"Removing {idx_nonetype} due to Nonetypes\")\n",
    "\n",
    "# Drop single atoms\n",
    "idx_single = [i for i,s in enumerate(test_df['SMILES']) if Chem.MolFromSmiles(s).GetNumAtoms() == 1 or '.' in s]\n",
    "test_df = test_df.drop(idx_single)\n",
    "if len(idx_single) > 0:\n",
    "    print(f\"Removing {idx_single} due to single atoms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144f5af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(data_df, test_size=0.1, random_state=0)\n",
    "train_loader = construct_loader(train_df, 'SmilesCurated', 'ExperimentalLogS', shuffle=True, batch_size=batch_size)\n",
    "val_loader = construct_loader(val_df, 'SmilesCurated', 'ExperimentalLogS', shuffle=False, batch_size=batch_size)\n",
    "test_loader = construct_loader(test_df, 'SMILES', 'LogS', shuffle=False, batch_size=1)\n",
    "print(f\"Train size: {len(train_loader.dataset)}, Val size: {len(val_loader.dataset)}, Test size: {len(test_loader.dataset)}\")\n",
    "\n",
    "# Standardizer for the solubility labels\n",
    "mean = np.mean(train_loader.dataset.labels)\n",
    "std = np.std(train_loader.dataset.labels)\n",
    "stdzer = Standardizer(mean, std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5304ee4a",
   "metadata": {},
   "source": [
    "### Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2385416",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparam_opt = True # Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99b3607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional hyperparameter optimization on the validation set\n",
    "\n",
    "def objective(trial):\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-2, log=True)\n",
    "    alpha = trial.suggest_float(\"alpha\", 1e-3, 1.0, log=True)\n",
    "    hidden_size = trial.suggest_int(\"hidden_size\", 64, 512, step=64)\n",
    "    depth = trial.suggest_int(\"depth\", 1, 7, step=1)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.0, 0.8, step=0.1)\n",
    "\n",
    "    model = GNN(train_loader.dataset.num_node_features, train_loader.dataset.num_edge_features, hidden_size=hidden_size, depth=depth, dropout=dropout).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss = nn.MSELoss(reduction='sum')\n",
    "\n",
    "    for epoch in range(5):\n",
    "        total_train_loss, _, _ = train_epoch(model, train_loader, optimizer, loss, alpha, stdzer)\n",
    "\n",
    "    preds = pred(model, val_loader, mode='predict', stdzer=stdzer)\n",
    "    pred_val_loss = root_mean_squared_error(preds, val_loader.dataset.labels)\n",
    "\n",
    "    return pred_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e483b18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if hyperparam_opt:    \n",
    "    study = optuna.create_study(direction=\"minimize\", pruner=optuna.pruners.HyperbandPruner(), study_name=\"hyperparam_opt\")\n",
    "    study.optimize(objective, n_trials=50, n_jobs=4)\n",
    "\n",
    "    print(f\"Best learning rate: {study.best_params['learning_rate']:.4g}\")\n",
    "    print(f\"Best alpha: {study.best_params['alpha']:.4g}\")\n",
    "    print(f\"Best hidden size: {study.best_params['hidden_size']:.4g}\")\n",
    "    print(f\"Best depth: {study.best_params['depth']:.4g}\")\n",
    "    print(f\"Best dropout: {study.best_params['dropout']:.4g}\")\n",
    "\n",
    "    study_results = study.trials_dataframe()\n",
    "    study_results.to_csv(\"hyperparam_opt.csv\", index=False)\n",
    "    print(\"Study results saved to hyperparam_opt.csv\")\n",
    "\n",
    "    learning_rate = study.best_params['learning_rate']\n",
    "    alpha = study.best_params['alpha']\n",
    "    hidden_size = study.best_params['hidden_size']\n",
    "    depth = study.best_params['depth']\n",
    "    dropout = study.best_params['dropout']\n",
    "\n",
    "# Standard hyperparameters are the result of a previous hyperparameter optimization\n",
    "else:\n",
    "    learning_rate = 0.0064\n",
    "    alpha = 0.002 # Weighting the losses of the two tasks\n",
    "    hidden_size = 384\n",
    "    depth = 3\n",
    "    dropout = 0.1\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "# If wanted, we can load an already trained model\n",
    "save_model = False  # Set to False if you don't want to save\n",
    "load_trained_model = False  # Set to True to skip training and load existing model\n",
    "save_plots = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9223e9",
   "metadata": {},
   "source": [
    "### Training on both Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014f216f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GNN(train_loader.dataset.num_node_features, train_loader.dataset.num_edge_features, hidden_size=hidden_size, depth=depth, dropout=dropout).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss = nn.MSELoss(reduction='sum')\n",
    "print('\\n', model, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaba426e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If wanted, we can load an already trained model\n",
    "\n",
    "# Path for the best model\n",
    "model_path = os.path.join(\"trained_models\", \"model.pt\")\n",
    "if load_trained_model and os.path.exists(model_path):\n",
    "    # Load pretrained model\n",
    "    print(f\"Loading model from {model_path}\")\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    best_model = deepcopy(model).to(device)\n",
    "    # Skip training\n",
    "    epochs = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bbd0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model on both tasks simultaneously\n",
    "if not load_trained_model:\n",
    "    best_model = deepcopy(model).to(device)\n",
    "    best_pred_val_loss = 1e5\n",
    "\n",
    "    total_train_loss_list = []\n",
    "    denoise_train_loss_list = []\n",
    "    pred_train_loss_list = []\n",
    "\n",
    "    total_val_loss_list = []\n",
    "    denoise_val_loss_list = []\n",
    "    pred_val_loss_list = []\n",
    "\n",
    "    if epochs == 0:\n",
    "        print(\"Skipping training, previously trained model was loaded.\")\n",
    "\n",
    "    for epoch in range(0, epochs):\n",
    "        total_train_loss, denoise_train_loss, pred_train_loss = train_epoch(model, train_loader, optimizer, loss, alpha=alpha, stdzer=stdzer)\n",
    "\n",
    "        denoised = pred(model, val_loader, mode='denoise')\n",
    "        node_feature_targets = [feature for batch in val_loader for feature in batch.x.cpu().flatten().tolist()]\n",
    "        edge_feature_targets = [feature for batch in val_loader for feature in batch.edge_attr.cpu().flatten().tolist()]\n",
    "        denoise_val_loss = root_mean_squared_error(denoised, node_feature_targets + edge_feature_targets)\n",
    "\n",
    "        preds = pred(model, val_loader, mode='predict', stdzer=stdzer)\n",
    "        pred_val_loss = root_mean_squared_error(preds, val_loader.dataset.labels)\n",
    "        total_val_loss = alpha * denoise_val_loss + (1 - alpha) * pred_val_loss\n",
    "\n",
    "        print(f\"Epoch {epoch}  Train Total Loss: {total_train_loss:.2f}  Train Denoise Loss: {denoise_train_loss:.2f}  Train Pred Loss: {pred_train_loss:.2f}  Val Total Loss: {total_val_loss:.2f}  Val Denoise Loss: {denoise_val_loss:.2f}  Val Pred Loss: {pred_val_loss:.2f}\")\n",
    "\n",
    "        total_train_loss_list.append(total_train_loss)\n",
    "        denoise_train_loss_list.append(denoise_train_loss)\n",
    "        pred_train_loss_list.append(pred_train_loss)\n",
    "        denoise_val_loss_list.append(denoise_val_loss)\n",
    "        pred_val_loss_list.append(pred_val_loss)\n",
    "        total_val_loss_list.append(total_val_loss)\n",
    "\n",
    "        if pred_val_loss < best_pred_val_loss:\n",
    "            best_model = deepcopy(model).to(device)\n",
    "            best_pred_val_loss = pred_val_loss\n",
    "            \n",
    "            # Save the best model\n",
    "            if save_model:\n",
    "                print(f\"Saving best model based on Val Pred Loss...\")\n",
    "                torch.save(best_model.state_dict(), os.path.join(\"trained_models\", f\"model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5694208f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only plot if actual training was done\n",
    "if epochs != 0:\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(7, 4))\n",
    "\n",
    "\n",
    "    # Just plotting the losses\n",
    "    ax.plot(list(range(epochs)), total_train_loss_list, label='Total Train Loss', color='blue')\n",
    "    ax.plot(list(range(epochs)), denoise_train_loss_list, label='Denoise Train Loss', color='orange')\n",
    "    ax.plot(list(range(epochs)), pred_train_loss_list, label='Pred Train Loss', color='green')\n",
    "    ax.plot(list(range(epochs)), total_val_loss_list, label='Total Val Loss', color='blue', linestyle='dashed')\n",
    "    ax.plot(list(range(epochs)), denoise_val_loss_list, label='Denoise Val Loss', color='orange', linestyle='dashed')\n",
    "    ax.plot(list(range(epochs)), pred_val_loss_list, label='Pred Val Loss', color='green', linestyle='dashed')\n",
    "    ax.set_title('Training and Validation Losses for both Tasks')\n",
    "    ax.legend(ncol=2)\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.set_xticks(list(range(0, epochs, 5)))\n",
    "    ax.set_ylabel('Loss')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if save_plots:   \n",
    "        fig.savefig(\"figures/loss_plot.jpg\", dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d473ff",
   "metadata": {},
   "source": [
    "### Training without SSL for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc13369",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_non_SSL = GNN(train_loader.dataset.num_node_features, train_loader.dataset.num_edge_features, hidden_size=hidden_size, depth=depth, dropout=dropout).to(device)\n",
    "loss_non_SSL = nn.MSELoss(reduction='sum')\n",
    "print('\\n', model, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e89e60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If wanted, we can load an already trained model\n",
    "\n",
    "# Path for the best model\n",
    "model_path = os.path.join(\"trained_models\", \"model_non_SSL.pt\")\n",
    "if load_trained_model and os.path.exists(model_path):\n",
    "    # Load pretrained model\n",
    "    print(f\"Loading model from {model_path}\")\n",
    "    model_non_SSL.load_state_dict(torch.load(model_path))\n",
    "    best_model_non_SSL = deepcopy(model_non_SSL).to(device)\n",
    "    # Skip training\n",
    "    epochs = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c22665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different optimizers for different tasks\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "best_model_non_SSL = deepcopy(model_non_SSL).to(device)\n",
    "best_val_loss = 1e5\n",
    "\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "\n",
    "if epochs == 0:\n",
    "    print(\"Skipping training, previously trained model was loaded.\")\n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    train_loss = train_epoch_without_SSL(model, train_loader, optimizer, loss, alpha=alpha, stdzer=stdzer)\n",
    "\n",
    "    preds = pred(model, val_loader, mode='predict', stdzer=stdzer)\n",
    "    val_loss = root_mean_squared_error(preds, val_loader.dataset.labels)\n",
    "\n",
    "    print(f\"Epoch {epoch}  Train Loss: {train_loss:.2f}  Val Loss: {val_loss:.2f}\")\n",
    "\n",
    "    train_loss_list.append(train_loss)\n",
    "    val_loss_list.append(val_loss)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_model_non_SSL = deepcopy(model_non_SSL).to(device)\n",
    "        best_val_loss = val_loss\n",
    "        \n",
    "        # Save the best model\n",
    "        if save_model:\n",
    "            print(f\"Saving best model...\")\n",
    "            torch.save(best_model_non_SSL.state_dict(), os.path.join(\"trained_models\", f\"model_non_SSL.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1a9dd0",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e771b4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_with_TTA(model, loader, alpha: float, lr: float, stdzer:Standardizer=None):\n",
    "    # Predict with test-time adaptation (TTA)\n",
    "    # We want a batch size of 1 for this\n",
    "\n",
    "    # Unfreeze the encoder\n",
    "    for param in model.encoder.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    # Unfreeze the decoder\n",
    "    for param in model.decoder.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    # Freeze the prediction head\n",
    "    for param in model.head.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    model = deepcopy(model).to(device)\n",
    "    model_before_step = deepcopy(model).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    preds = []\n",
    "\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        model.set_mode('denoise')\n",
    "        model.train()\n",
    "        \n",
    "        node_out, edge_out = model(batch)\n",
    "        node_loss = loss(node_out, batch.x)\n",
    "        edge_loss = loss(edge_out, batch.edge_attr)\n",
    "        # Losses get the same weighting as in the training step\n",
    "        denoise_loss = alpha * (node_loss + edge_loss)\n",
    "        denoise_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        model.set_mode('predict')\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = model(batch)\n",
    "            pred = stdzer(out, rev=True)\n",
    "            preds.extend(pred.cpu().detach().tolist())\n",
    "\n",
    "        model = deepcopy(model_before_step)\n",
    "        \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ee276b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction with non-SSL model\n",
    "preds_non_SSL = pred(model_non_SSL, test_loader, mode='predict', stdzer=stdzer)\n",
    "print('Reference results on test set for model trained without SSL:')\n",
    "print(f\"RMSE: {root_mean_squared_error(preds_non_SSL, test_loader.dataset.labels):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04a3ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction with SLL model without TTA\n",
    "preds = pred(best_model, test_loader, mode='predict', stdzer=stdzer)\n",
    "print('Results on test set without TTA:')  \n",
    "print(f\"RMSE: {root_mean_squared_error(preds, test_loader.dataset.labels):.4g}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad11abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional hyperparameter optimization on the validation set for the TTA step\n",
    "\n",
    "def objective(trial):\n",
    "    learning_rate_TTA = trial.suggest_float(\"learning_rate_TTA\", 1e-5, 1.0, log=True)\n",
    "    alpha_TTA = trial.suggest_float(\"alpha_TTA\", 1e-4, 1.0, log=True)\n",
    "    preds_TTA = pred_with_TTA(best_model, val_loader, alpha=alpha_TTA, lr=learning_rate_TTA, stdzer=stdzer)\n",
    "\n",
    "    return root_mean_squared_error(preds_TTA, val_loader.dataset.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35a72f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if hyperparam_opt:    \n",
    "    study = optuna.create_study(direction=\"minimize\", pruner=optuna.pruners.HyperbandPruner(), study_name=\"TTA_hyperparam_opt\")\n",
    "    study.optimize(objective, n_trials=100, n_jobs=6)\n",
    "\n",
    "    print(f\"Best TTA learning rate: {study.best_params['learning_rate_TTA']:.4g}\")\n",
    "    print(f\"Best TTA alpha: {study.best_params['alpha_TTA']:.4g}\")\n",
    "\n",
    "    study_results = study.trials_dataframe()\n",
    "    study_results.to_csv(\"TTA_hyperparam_opt.csv\", index=False)\n",
    "    print(\"Study results saved to TTA_hyperparam_opt.csv\")\n",
    "\n",
    "    learning_rate_TTA = study.best_params['learning_rate_TTA']\n",
    "    alpha_TTA = study.best_params['alpha_TTA']\n",
    "\n",
    "# Standard hyperparameters are the result of a previous hyperparameter optimization\n",
    "else:\n",
    "    learning_rate_TTA = 0.00175\n",
    "    alpha_TTA = 0.0958"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54c4abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction with SLL model with TTA\n",
    "# Takes about three times as long as prediction without TTA\n",
    "preds_TTA = pred_with_TTA(best_model, test_loader, alpha=alpha_TTA, lr=learning_rate_TTA, stdzer=stdzer)\n",
    "print('Results on test set with TTA:')\n",
    "print(f\"RMSE: {root_mean_squared_error(preds_TTA, test_loader.dataset.labels):.4g}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ed4839",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e228728b",
   "metadata": {},
   "source": [
    "### Getting the Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38d0d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embeddings_with_TTA(model, loader, alpha: float, lr: float):\n",
    "    # Get embeddings with test-time adaptation (TTA)\n",
    "    # We want a batch size of 1 for this\n",
    "\n",
    "    # Unfreeze the encoder\n",
    "    for param in model.encoder.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    # Unfreeze the decoder\n",
    "    for param in model.decoder.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    # Freeze the prediction head\n",
    "    for param in model.head.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    model = deepcopy(model).to(device)\n",
    "    model_before_step = deepcopy(model).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    embeddings = []\n",
    "\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        model.set_mode('denoise')\n",
    "        model.train()\n",
    "        \n",
    "        node_out, edge_out = model(batch)\n",
    "        node_loss = loss(node_out, batch.x)\n",
    "        edge_loss = loss(edge_out, batch.edge_attr)\n",
    "        # Losses get the same weighting as in the training step\n",
    "        denoise_loss = alpha * (node_loss + edge_loss)\n",
    "        denoise_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        model.set_mode('predict')\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            batch = batch.to(device)\n",
    "            embedding = model.get_embedding(batch)\n",
    "            embeddings.extend(embedding.cpu().detach().numpy())\n",
    "\n",
    "        model = deepcopy(model_before_step)\n",
    "        \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78d4bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = best_model.to(device)\n",
    "\n",
    "# Getting embeddings for analysis using PCA\n",
    "# The embeddings are lists of length \"samples\" of vectors with length \"hidden_size\"\n",
    "train_embeddings = []\n",
    "for batch in train_loader:\n",
    "    batch = batch.to(device)\n",
    "    embedding = best_model.get_embedding(batch)\n",
    "    train_embeddings.extend(embedding.cpu().detach().numpy())\n",
    "\n",
    "val_embeddings = []\n",
    "for batch in val_loader:\n",
    "    batch = batch.to(device)\n",
    "    embedding = best_model.get_embedding(batch)\n",
    "    val_embeddings.extend(embedding.cpu().detach().numpy())\n",
    "\n",
    "test_embeddings = []\n",
    "for batch in test_loader:\n",
    "    batch = batch.to(device)\n",
    "    embedding = best_model.get_embedding(batch)\n",
    "    test_embeddings.extend(embedding.cpu().detach().numpy())\n",
    "\n",
    "# Create and fit PCA\n",
    "pca = PCA(n_components=2)\n",
    "train_embeddings_2d = pca.fit_transform(np.array(train_embeddings))\n",
    "val_embeddings_2d = pca.transform(np.array(val_embeddings))\n",
    "test_embeddings_2d = pca.transform(np.array(test_embeddings))\n",
    "print(f\"Explained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "\n",
    "# Also getting the test set embeddings after TTA\n",
    "test_embeddings_with_TTA = embeddings_with_TTA(best_model, test_loader, alpha=alpha_TTA, lr=learning_rate_TTA)\n",
    "test_embeddings_with_TTA_2d = pca.transform(np.array(test_embeddings_with_TTA))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7d2cf6",
   "metadata": {},
   "source": [
    "### Analyzing the Solubility Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28a54a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the solubility values from each dataset\n",
    "train_solubility = train_loader.dataset.labels\n",
    "val_solubility = val_loader.dataset.labels\n",
    "test_solubility = test_loader.dataset.labels\n",
    "\n",
    "# Create a single figure and axes\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "\n",
    "# Define a consistent colormap for solubility\n",
    "cmap = plt.cm.viridis\n",
    "norm = plt.Normalize(min(min(train_solubility), min(val_solubility), min(test_solubility)),\n",
    "                     max(max(train_solubility), max(val_solubility), max(test_solubility)))\n",
    "\n",
    "# Plot all datasets on the same axes with different markers\n",
    "sc1 = ax.scatter(train_embeddings_2d[:, 0], train_embeddings_2d[:, 1], \n",
    "                c=train_solubility, cmap=cmap, norm=norm, alpha=0.5, s=3)\n",
    "sc2 = ax.scatter(val_embeddings_2d[:, 0], val_embeddings_2d[:, 1], \n",
    "                c=val_solubility, cmap=cmap, norm=norm, alpha=0.5, s=3)\n",
    "sc3 = ax.scatter(test_embeddings_2d[:, 0], test_embeddings_2d[:, 1], \n",
    "                c=test_solubility, cmap=cmap, norm=norm, alpha=0.5, s=3)\n",
    "\n",
    "# Add a colorbar, title, labels, and legend\n",
    "cbar = fig.colorbar(sc1, ax=ax, aspect=50)\n",
    "cbar.set_label('Solubility')\n",
    "ax.set_title('PCA Projection of Embedding Vectors with Solubility')\n",
    "ax.set_xlabel('Principal Component 1')\n",
    "ax.set_ylabel('Principal Component 2')\n",
    "\n",
    "# Dynamically adjust the limits to focus on the main data distribution\n",
    "# Use percentiles to avoid sensitivity to outliers\n",
    "x_min_train, x_max_train = np.percentile(train_embeddings_2d[:, 0], [5, 95])\n",
    "y_min_train, y_max_train = np.percentile(train_embeddings_2d[:, 1], [5, 95])\n",
    "x_min_test, x_max_test = np.percentile(test_embeddings_2d[:, 0], [5, 95])\n",
    "y_min_test, y_max_test = np.percentile(test_embeddings_2d[:, 1], [5, 95])\n",
    "x_max = max(x_max_train, x_max_test)\n",
    "x_min = min(x_min_train, x_min_test)\n",
    "y_max = max(y_max_train, y_max_test)\n",
    "y_min = min(y_min_train, y_min_test)\n",
    "# Add a small margin to the limits\n",
    "margin_x = (x_max - x_min) * 0.05\n",
    "margin_y = (y_max - y_min) * 0.05\n",
    "\n",
    "ax.set_xlim(x_min - margin_x, x_max + margin_x)\n",
    "ax.set_ylim(y_min - margin_y, y_max + margin_y)\n",
    "\n",
    "plt.tight_layout()\n",
    "if save_plots:  \n",
    "    fig.savefig(\"figures/pca_solubility.jpg\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbbea10",
   "metadata": {},
   "source": [
    "### Analyzing the Class Distribution and Effect of TTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a43d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the 2D embeddings using fig, ax\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "\n",
    "ax.scatter(train_embeddings_2d[:, 0], train_embeddings_2d[:, 1], alpha=0.5, label='Train Set', s=3)\n",
    "ax.scatter(val_embeddings_2d[:, 0], val_embeddings_2d[:, 1], alpha=0.5, label='Validation Set', s=3)\n",
    "ax.scatter(test_embeddings_2d[:, 0], test_embeddings_2d[:, 1], alpha=0.5, label='Test Set', s=3)\n",
    "ax.scatter(test_embeddings_with_TTA_2d[:, 0], test_embeddings_with_TTA_2d[:, 1], alpha=0.5, label='Test Set with TTA', s=3)\n",
    "\n",
    "# Calculate centroids with median insetad of mean since we have outliers\n",
    "train_centroid = np.median(train_embeddings_2d, axis=0)\n",
    "val_centroid = np.median(val_embeddings_2d, axis=0)\n",
    "test_centroid = np.median(test_embeddings_2d, axis=0)\n",
    "test_tta_centroid = np.median(test_embeddings_with_TTA_2d, axis=0)\n",
    "\n",
    "# Plot centroids with larger markers\n",
    "ax.scatter(train_centroid[0], train_centroid[1], s=100, \n",
    "           c='blue', marker='+', edgecolors='white', linewidths=2, label='Train Centroid', alpha=0.9)\n",
    "ax.scatter(val_centroid[0], val_centroid[1], s=100, \n",
    "           c='orange', marker='x', edgecolors='black', linewidths=2, label='Val Centroid', alpha=0.9)\n",
    "ax.scatter(test_centroid[0], test_centroid[1], s=100, \n",
    "           c='green', marker='+', edgecolors='black', linewidths=2, label='Test Centroid', alpha=0.9)\n",
    "ax.scatter(test_tta_centroid[0], test_tta_centroid[1], s=100, \n",
    "           c='red', marker='x', edgecolors='black', linewidths=2, label='Test Centroid with TTA', alpha=0.9)\n",
    "\n",
    "ax.legend()\n",
    "ax.set_title('PCA Projection of Embedding with Sets')\n",
    "ax.set_xlabel('Principal Component 1')\n",
    "ax.set_ylabel('Principal Component 2')\n",
    "ax.set_xlim(x_min - margin_x, x_max + margin_x)\n",
    "ax.set_ylim(y_min - margin_y, y_max + margin_y)\n",
    "\n",
    "plt.tight_layout()\n",
    "if save_plots:  \n",
    "    fig.savefig(\"figures/pca_sets.jpg\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc472a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the 2D embeddings using fig, ax\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "ax.scatter(test_embeddings_2d[:, 0], test_embeddings_2d[:, 1], alpha=0.5, label='Test Set without TTA', s=3)\n",
    "ax.scatter(test_embeddings_with_TTA_2d[:, 0], test_embeddings_with_TTA_2d[:, 1], alpha=0.5, label='Test Set with TTA', s=3)\n",
    "\n",
    "# Plot centroids with larger markers\n",
    "ax.scatter(train_centroid[0], train_centroid[1], s=100, \n",
    "           c='blue', marker='+', edgecolors='white', linewidths=2, label='Train Centroid', alpha=0.9)\n",
    "ax.scatter(val_centroid[0], val_centroid[1], s=100, \n",
    "           c='orange', marker='x', edgecolors='black', linewidths=2, label='Val Centroid', alpha=0.9)\n",
    "ax.scatter(test_centroid[0], test_centroid[1], s=100, \n",
    "           c='green', marker='+', edgecolors='black', linewidths=2, label='Test Centroid', alpha=0.9)\n",
    "ax.scatter(test_tta_centroid[0], test_tta_centroid[1], s=100, \n",
    "           c='red', marker='x', edgecolors='black', linewidths=2, label='Test Centroid with TTA', alpha=0.9)\n",
    "\n",
    "ax.legend()\n",
    "ax.set_title('PCA Projection of Embedding with TTA vs No TTA')\n",
    "ax.set_xlabel('Principal Component 1')\n",
    "ax.set_ylabel('Principal Component 2')\n",
    "ax.set_xlim(x_min - margin_x, x_max + margin_x)\n",
    "ax.set_ylim(y_min - margin_y, y_max + margin_y)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig(\"figures/pca_sets_TTA_vs_no_TTA.jpg\", dpi=300)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
