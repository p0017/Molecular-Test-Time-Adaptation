{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7ec02f-bc61-4c89-987d-b3025b2a474f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from rdkit import Chem\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n",
    "from umap import UMAP\n",
    "import torch\n",
    "from torch import nn\n",
    "import optuna\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09884a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import custom modules\n",
    "from utils.data_utils import construct_loader\n",
    "from utils.model_utils import GNN\n",
    "from utils.train_test_utils import (\n",
    "    Standardizer,\n",
    "    train_epoch,\n",
    "    train_epoch_without_SSL,\n",
    "    pred,\n",
    "    pred_with_TTA,\n",
    "    embeddings_with_TTA,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089eeedd",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0998c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "epochs = 20\n",
    "\n",
    "train_hyperparam_opt = False # Optional optimization of training and testing hyperparameters\n",
    "test_hyperparam_opt = False # Optional optimization of testing hyperparameters\n",
    "\n",
    "# If wanted, we can load an already trained model\n",
    "save_model = False  # Set to False if you don't want to save\n",
    "load_trained_model = True  # Set to True to skip training and load existing model\n",
    "save_plots = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f33b3f",
   "metadata": {},
   "source": [
    "### Loading and preprocessing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e699e0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv(\"data/AqSolDBc.csv\")\n",
    "# Drop single atoms\n",
    "idx_single = [\n",
    "    i\n",
    "    for i, s in enumerate(data_df[\"SmilesCurated\"])\n",
    "    if Chem.MolFromSmiles(s).GetNumAtoms() == 1 or \".\" in s\n",
    "]\n",
    "\n",
    "data_df = data_df.drop(idx_single)\n",
    "if len(idx_single) > 0:\n",
    "    print(f\"Removing {idx_single} due to single atoms\")\n",
    "\n",
    "test_df = pd.read_csv(\"data/OChemUnseen.csv\")\n",
    "# Drop some Nonetypes\n",
    "# Got an error for a SMILES which was None\n",
    "idx_nonetype = [\n",
    "    i for i, s in enumerate(test_df[\"SMILES\"]) if Chem.MolFromSmiles(s) is None\n",
    "]\n",
    "\n",
    "test_df = test_df.drop(idx_nonetype)\n",
    "if len(idx_nonetype) > 0:\n",
    "    print(f\"Removing {idx_nonetype} due to Nonetypes\")\n",
    "\n",
    "# Drop single atoms\n",
    "idx_single = [\n",
    "    i\n",
    "    for i, s in enumerate(test_df[\"SMILES\"])\n",
    "    if Chem.MolFromSmiles(s).GetNumAtoms() == 1 or \".\" in s\n",
    "]\n",
    "\n",
    "test_df = test_df.drop(idx_single)\n",
    "if len(idx_single) > 0:\n",
    "    print(f\"Removing {idx_single} due to single atoms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144f5af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(data_df, test_size=0.1, random_state=0)\n",
    "train_loader = construct_loader(\n",
    "    train_df, \"SmilesCurated\", \"ExperimentalLogS\", shuffle=True, batch_size=batch_size\n",
    ")\n",
    "\n",
    "val_loader = construct_loader(\n",
    "    val_df, \"SmilesCurated\", \"ExperimentalLogS\", shuffle=False, batch_size=batch_size\n",
    ")\n",
    "\n",
    "test_loader = construct_loader(test_df, \"SMILES\", \"LogS\", shuffle=False, batch_size=1)\n",
    "print(\n",
    "    f\"Train size: {len(train_loader.dataset)}, Val size: {len(val_loader.dataset)}, Test size: {len(test_loader.dataset)}\"\n",
    ")\n",
    "\n",
    "# Standardizer for the solubility labels\n",
    "mean = np.mean(train_loader.dataset.labels)\n",
    "std = np.std(train_loader.dataset.labels)\n",
    "stdzer = Standardizer(mean, std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5304ee4a",
   "metadata": {},
   "source": [
    "### Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99b3607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional hyperparameter optimization on the validation set\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-4, 5e-3, log=True)\n",
    "    # Weighting the losses of the two tasks\n",
    "    alpha = trial.suggest_float(\"alpha\", 0.3, 0.6, step=0.1)  \n",
    "    # Size of the encoder-decoder bottleneck\n",
    "    hidden_size = trial.suggest_categorical(\"hidden_size\", [16, 32])  \n",
    "    depth = trial.suggest_categorical(\"depth\", [3, 4, 5])\n",
    "    dropout = trial.suggest_categorical(\"dropout\", [0.1, 0.2, 0.3])\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 5e-5, log=True)\n",
    "\n",
    "    model = GNN(\n",
    "        train_loader.dataset.num_node_features,\n",
    "        train_loader.dataset.num_edge_features,\n",
    "        hidden_size=hidden_size,\n",
    "        depth=depth,\n",
    "        dropout=dropout,\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=learning_rate, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    loss = nn.MSELoss(reduction=\"mean\")\n",
    "\n",
    "    # By default, we train for a fourth of the epochs used in the main training to get a rough estimate\n",
    "    for epoch in range(epochs // 2):\n",
    "        train_epoch(model, train_loader, optimizer, loss, alpha, stdzer)\n",
    "\n",
    "        preds = pred(model, val_loader, mode=\"predict\", stdzer=stdzer)\n",
    "        rmse_pred = root_mean_squared_error(preds, val_loader.dataset.labels)\n",
    "\n",
    "        trial.report(rmse_pred, epoch)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return rmse_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e483b18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes about half an hour on my machine for a setting of 20 epochs\n",
    "if train_hyperparam_opt:\n",
    "    study = optuna.create_study(\n",
    "        direction=\"minimize\",\n",
    "        pruner=optuna.pruners.HyperbandPruner(),\n",
    "        study_name=\"hyperparam_opt\",\n",
    "    )\n",
    "    \n",
    "    study.optimize(objective, n_trials=40, n_jobs=4)\n",
    "\n",
    "    print(f\"Best learning rate: {study.best_params['learning_rate']:.4f}\")\n",
    "    print(f\"Best alpha: {study.best_params['alpha']:.4f}\")\n",
    "    print(f\"Best hidden size: {study.best_params['hidden_size']:.4f}\")\n",
    "    print(f\"Best depth: {study.best_params['depth']:.4f}\")\n",
    "    print(f\"Best dropout: {study.best_params['dropout']:.4f}\")\n",
    "    print(f\"Best weight decay: {study.best_params['weight_decay']:.4f}\")\n",
    "\n",
    "    study_results = study.trials_dataframe()\n",
    "    study_results.to_csv(\"data/hyperparam_opt.csv\", index=False)\n",
    "    print(\"Study results saved to hyperparam_opt.csv\")\n",
    "\n",
    "    learning_rate = study.best_params[\"learning_rate\"]\n",
    "    alpha = study.best_params[\"alpha\"]\n",
    "    hidden_size = study.best_params[\"hidden_size\"]\n",
    "    depth = study.best_params[\"depth\"]\n",
    "    dropout = study.best_params[\"dropout\"]\n",
    "    weight_decay = study.best_params[\"weight_decay\"]\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Standard hyperparameters are the result of a previous hyperparameter optimization\n",
    "else:\n",
    "    learning_rate = 0.0026\n",
    "    alpha = 0.3  # Weighting the losses of the two tasks, smaller alpha means less weight on the denoising task\n",
    "    hidden_size = 32\n",
    "    depth = 3\n",
    "    dropout = 0.3\n",
    "    weight_decay = 5e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9223e9",
   "metadata": {},
   "source": [
    "### Training on both Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014f216f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GNN(\n",
    "    train_loader.dataset.num_node_features,\n",
    "    train_loader.dataset.num_edge_features,\n",
    "    hidden_size=hidden_size,\n",
    "    depth=depth,\n",
    "    dropout=dropout,\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=learning_rate, weight_decay=weight_decay\n",
    ")\n",
    "\n",
    "loss = nn.MSELoss(reduction=\"mean\")\n",
    "print(\"\\n\", model, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaba426e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If wanted, we can load an already trained model\n",
    "\n",
    "# Path for the best model\n",
    "model_path = os.path.join(\"trained_models\", \"model.pt\")\n",
    "if load_trained_model and os.path.exists(model_path):\n",
    "    # Load pretrained model\n",
    "    print(f\"Loading model from {model_path}\")\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    best_model = deepcopy(model).to(device)\n",
    "    # Skip training\n",
    "    epochs = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bbd0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model on both tasks simultaneously\n",
    "if not load_trained_model:\n",
    "    best_model = deepcopy(model).to(device)\n",
    "    best_combined_val_loss = 1e5\n",
    "\n",
    "    combined_train_loss_list = []\n",
    "    denoise_train_loss_list = []\n",
    "    pred_train_loss_list = []\n",
    "\n",
    "    combined_val_loss_list = []\n",
    "    denoise_val_loss_list = []\n",
    "    pred_val_loss_list = []\n",
    "\n",
    "    if epochs == 0:\n",
    "        print(\"Skipping training, previously trained model was loaded.\")\n",
    "\n",
    "    for epoch in range(0, epochs):\n",
    "        combined_train_loss, denoise_train_loss, pred_train_loss = train_epoch(\n",
    "            model, train_loader, optimizer, loss, alpha=alpha, stdzer=stdzer\n",
    "        )\n",
    "\n",
    "        denoised = pred(model, val_loader, mode=\"denoise\", stdzer=stdzer)\n",
    "        node_feature_targets = [\n",
    "            feature\n",
    "            for batch in val_loader\n",
    "            for feature in batch.x.cpu().flatten().tolist()\n",
    "        ]\n",
    "        \n",
    "        edge_feature_targets = [\n",
    "            feature\n",
    "            for batch in val_loader\n",
    "            for feature in batch.edge_attr.cpu().flatten().tolist()\n",
    "        ]\n",
    "\n",
    "        denoise_val_loss = root_mean_squared_error(\n",
    "            denoised, node_feature_targets + edge_feature_targets\n",
    "        )\n",
    "\n",
    "        preds = pred(model, val_loader, mode=\"predict\", stdzer=stdzer)\n",
    "        pred_val_loss = root_mean_squared_error(preds, val_loader.dataset.labels)\n",
    "        combined_val_loss = alpha * denoise_val_loss + (1 - alpha) * pred_val_loss\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch}  Train Combined Loss: {combined_train_loss:.3f}  Train Denoise Loss: {denoise_train_loss:.3f}  Train Pred Loss: {pred_train_loss:.3f}  Val Combined Loss: {combined_val_loss:.3f}  Val Denoise Loss: {denoise_val_loss:.3f}  Val Pred Loss: {pred_val_loss:.3f}\"\n",
    "        )\n",
    "\n",
    "        combined_train_loss_list.append(combined_train_loss)\n",
    "        denoise_train_loss_list.append(denoise_train_loss)\n",
    "        pred_train_loss_list.append(pred_train_loss)\n",
    "        denoise_val_loss_list.append(denoise_val_loss)\n",
    "        pred_val_loss_list.append(pred_val_loss)\n",
    "        combined_val_loss_list.append(combined_val_loss)\n",
    "\n",
    "\n",
    "        if combined_val_loss < best_combined_val_loss:\n",
    "            best_model = deepcopy(model).to(device)\n",
    "            best_combined_val_loss = combined_val_loss\n",
    "\n",
    "            # Save the best model\n",
    "            if save_model:\n",
    "                print(f\"Saving best model based on combined Val Loss...\")\n",
    "                torch.save(\n",
    "                    best_model.state_dict(), os.path.join(\"trained_models\", f\"model.pt\")\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5694208f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only plot if actual training was done\n",
    "if epochs != 0:\n",
    "\n",
    "    def normalize_losses(loss_list):\n",
    "        # Normalize losses to [0, 1]\n",
    "        min_val, max_val = min(loss_list), max(loss_list)\n",
    "        return [(loss - min_val) / (max_val - min_val) for loss in loss_list]\n",
    "\n",
    "    combined_train_loss_list_norm = normalize_losses(combined_train_loss_list)\n",
    "    denoise_train_loss_list_norm = normalize_losses(denoise_train_loss_list)\n",
    "    pred_train_loss_list_norm = normalize_losses(pred_train_loss_list)\n",
    "    combined_val_loss_list_norm = normalize_losses(combined_val_loss_list)\n",
    "    denoise_val_loss_list_norm = normalize_losses(denoise_val_loss_list)\n",
    "    pred_val_loss_list_norm = normalize_losses(pred_val_loss_list)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "\n",
    "    # Plot normalized losses\n",
    "    ax.plot(\n",
    "        list(range(epochs)),\n",
    "        combined_train_loss_list_norm,\n",
    "        label=\"Combined Train Loss\",\n",
    "        color=\"blue\",\n",
    "    )\n",
    "    ax.plot(\n",
    "        list(range(epochs)),\n",
    "        denoise_train_loss_list_norm,\n",
    "        label=\"Denoise Train Loss\",\n",
    "        color=\"orange\",\n",
    "    )\n",
    "    ax.plot(\n",
    "        list(range(epochs)),\n",
    "        pred_train_loss_list_norm,\n",
    "        label=\"Pred Train Loss\",\n",
    "        color=\"green\",\n",
    "    )\n",
    "    ax.plot(\n",
    "        list(range(epochs)),\n",
    "        combined_val_loss_list_norm,\n",
    "        label=\"Combined Val Loss\",\n",
    "        color=\"blue\",\n",
    "        linestyle=\"dashed\",\n",
    "    )\n",
    "    ax.plot(\n",
    "        list(range(epochs)),\n",
    "        denoise_val_loss_list_norm,\n",
    "        label=\"Denoise Val Loss\",\n",
    "        color=\"orange\",\n",
    "        linestyle=\"dashed\",\n",
    "    )\n",
    "    ax.plot(\n",
    "        list(range(epochs)),\n",
    "        pred_val_loss_list_norm,\n",
    "        label=\"Pred Val Loss\",\n",
    "        color=\"green\",\n",
    "        linestyle=\"dashed\",\n",
    "    )\n",
    "    \n",
    "    ax.legend(ncol=2)\n",
    "    ax.set_xlabel(\"Epochs\")\n",
    "    ax.set_xticks(list(range(0, epochs, 5)))\n",
    "    ax.set_ylabel(\"Normalized Loss\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if save_plots:\n",
    "        plt.savefig(\"figures/loss_plot.jpg\", dpi=300, bbox_inches=\"tight\")\n",
    "        plt.savefig(\"figures/loss_plot.pdf\", bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d473ff",
   "metadata": {},
   "source": [
    "### Training without SSL for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc13369",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_non_SSL = GNN(\n",
    "    train_loader.dataset.num_node_features,\n",
    "    train_loader.dataset.num_edge_features,\n",
    "    hidden_size=hidden_size,\n",
    "    depth=depth,\n",
    "    dropout=dropout,\n",
    ").to(device)\n",
    "\n",
    "optimizer_non_SSL = torch.optim.Adam(\n",
    "    model.parameters(), lr=learning_rate, weight_decay=weight_decay\n",
    ")\n",
    "\n",
    "loss_non_SSL = nn.MSELoss(reduction=\"mean\")\n",
    "print(\"\\n\", model, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e89e60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If wanted, we can load an already trained model\n",
    "\n",
    "# Path for the best model\n",
    "model_path = os.path.join(\"trained_models\", \"model_non_SSL.pt\")\n",
    "if load_trained_model and os.path.exists(model_path):\n",
    "    # Load pretrained model\n",
    "    print(f\"Loading model from {model_path}\")\n",
    "    model_non_SSL.load_state_dict(torch.load(model_path))\n",
    "    best_model_non_SSL = deepcopy(model_non_SSL).to(device)\n",
    "    # Skip training\n",
    "    epochs = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c22665",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_non_SSL = deepcopy(model_non_SSL).to(device)\n",
    "best_val_loss = 1e5\n",
    "\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "\n",
    "if epochs == 0:\n",
    "    print(\"Skipping training, previously trained model was loaded.\")\n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    train_loss = train_epoch_without_SSL(\n",
    "        model_non_SSL,\n",
    "        train_loader,\n",
    "        optimizer_non_SSL,\n",
    "        loss_non_SSL,\n",
    "        alpha=alpha,\n",
    "        stdzer=stdzer,\n",
    "    )\n",
    "\n",
    "    preds = pred(model_non_SSL, val_loader, mode=\"predict\", stdzer=stdzer)\n",
    "    val_loss = root_mean_squared_error(preds, val_loader.dataset.labels)\n",
    "\n",
    "    print(f\"Epoch {epoch}  Train Loss: {train_loss:.2f}  Val Loss: {val_loss:.2f}\")\n",
    "\n",
    "    train_loss_list.append(train_loss)\n",
    "    val_loss_list.append(val_loss)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_model_non_SSL = deepcopy(model_non_SSL).to(device)\n",
    "        best_val_loss = val_loss\n",
    "\n",
    "        # Save the best model\n",
    "        if save_model:\n",
    "            print(f\"Saving best model based on Val Loss...\")\n",
    "            torch.save(\n",
    "                best_model_non_SSL.state_dict(),\n",
    "                os.path.join(\"trained_models\", f\"model_non_SSL.pt\"),\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1a9dd0",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad11abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional hyperparameter optimization on the validation set for the TTA step\n",
    "\n",
    "def objective(trial):\n",
    "    learning_rate_TTA = trial.suggest_float(\"learning_rate_TTA\", 1e-5, 1.0, log=True)\n",
    "\n",
    "    preds_TTA = pred_with_TTA(\n",
    "        best_model, val_loader, lr=learning_rate_TTA, stdzer=stdzer\n",
    "    )\n",
    "\n",
    "    return root_mean_squared_error(preds_TTA, val_loader.dataset.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35a72f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_hyperparam_opt:\n",
    "    study = optuna.create_study(\n",
    "        direction=\"minimize\",\n",
    "        pruner=optuna.pruners.HyperbandPruner(),\n",
    "        study_name=\"TTA_hyperparam_opt\",\n",
    "    )\n",
    "    \n",
    "    study.optimize(objective, n_trials=30, n_jobs=6)\n",
    "\n",
    "    print(f\"Best TTA learning rate: {study.best_params['learning_rate_TTA']:.5f}\")\n",
    "\n",
    "    study_results = study.trials_dataframe()\n",
    "    study_results.to_csv(\"data/TTA_hyperparam_opt.csv\", index=False)\n",
    "    print(\"Study results saved to TTA_hyperparam_opt.csv\")\n",
    "\n",
    "    learning_rate_TTA = study.best_params[\"learning_rate_TTA\"]\n",
    "\n",
    "# Standard hyperparameters are the result of a previous hyperparameter optimization\n",
    "else:\n",
    "    learning_rate_TTA = 0.00021  # It seems as the best TTA learning rate is about a tenth of the training learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ee276b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction with non-SSL model\n",
    "val_preds_non_SSL = pred(best_model_non_SSL, val_loader, mode=\"predict\", stdzer=stdzer)\n",
    "test_preds_non_SSL = pred(\n",
    "    best_model_non_SSL, test_loader, mode=\"predict\", stdzer=stdzer\n",
    ")\n",
    "\n",
    "print(\"Reference results for model trained without SSL:\")\n",
    "print(\n",
    "    f\"Val Set RMSE: {root_mean_squared_error(val_preds_non_SSL, val_loader.dataset.labels):.4f}\"\n",
    ")\n",
    "print(\n",
    "    f\"Test Set RMSE: {root_mean_squared_error(test_preds_non_SSL, test_loader.dataset.labels):.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04a3ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction with SLL model without TTA\n",
    "val_preds = pred(best_model, val_loader, mode=\"predict\", stdzer=stdzer)\n",
    "test_preds = pred(best_model, test_loader, mode=\"predict\", stdzer=stdzer)\n",
    "print(\"Results without TTA:\")\n",
    "print(\n",
    "    f\"Val Set RMSE: {root_mean_squared_error(val_preds, val_loader.dataset.labels):.4f}\"\n",
    ")\n",
    "print(\n",
    "    f\"Test Set RMSE: {root_mean_squared_error(test_preds, test_loader.dataset.labels):.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54c4abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction with SLL model with TTA\n",
    "# Takes about three times as long as prediction without TTA\n",
    "val_preds_TTA = pred_with_TTA(\n",
    "    best_model, val_loader, lr=learning_rate_TTA, stdzer=stdzer\n",
    ")\n",
    "test_preds_TTA = pred_with_TTA(\n",
    "    best_model, test_loader, lr=learning_rate_TTA, stdzer=stdzer\n",
    ")\n",
    "\n",
    "print(\"Results on with TTA:\")\n",
    "print(\n",
    "    f\"Val Set RMSE: {root_mean_squared_error(val_preds_TTA, val_loader.dataset.labels):.4f}\"\n",
    ")\n",
    "print(\n",
    "    f\"Test Set RMSE: {root_mean_squared_error(test_preds_TTA, test_loader.dataset.labels):.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ed4839",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e228728b",
   "metadata": {},
   "source": [
    "### Getting the Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78d4bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = best_model.to(device)\n",
    "\n",
    "# The embeddings are lists of length \"samples\" of vectors with length \"hidden_size\"\n",
    "train_embeddings = []\n",
    "for batch in train_loader:\n",
    "    batch = batch.to(device)\n",
    "    embedding = best_model.get_embedding(batch)\n",
    "    train_embeddings.extend(embedding.cpu().detach().numpy())\n",
    "\n",
    "val_embeddings = []\n",
    "for batch in val_loader:\n",
    "    batch = batch.to(device)\n",
    "    embedding = best_model.get_embedding(batch)\n",
    "    val_embeddings.extend(embedding.cpu().detach().numpy())\n",
    "\n",
    "test_embeddings = []\n",
    "for batch in test_loader:\n",
    "    batch = batch.to(device)\n",
    "    embedding = best_model.get_embedding(batch)\n",
    "    test_embeddings.extend(embedding.cpu().detach().numpy())\n",
    "\n",
    "# Combine all embeddings for t-SNE fitting\n",
    "all_embeddings = np.vstack([train_embeddings, val_embeddings, test_embeddings])\n",
    "\n",
    "# Create and fit t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "all_embeddings_tsne = tsne.fit_transform(all_embeddings)\n",
    "\n",
    "# Split back into individual datasets\n",
    "train_size = len(train_embeddings)\n",
    "val_size = len(val_embeddings)\n",
    "test_size = len(test_embeddings)\n",
    "\n",
    "train_embeddings_tsne = all_embeddings_tsne[:train_size]\n",
    "val_embeddings_tsne = all_embeddings_tsne[train_size : train_size + val_size]\n",
    "test_embeddings_tsne = all_embeddings_tsne[train_size + val_size :]\n",
    "\n",
    "# Create and fit UMAP for 2D projections\n",
    "umap_reducer = UMAP(\n",
    "    n_components=2,\n",
    ")\n",
    "all_embeddings_umap = umap_reducer.fit_transform(all_embeddings, n_jobs=-1)\n",
    "\n",
    "# Split UMAP results back into individual datasets\n",
    "train_embeddings_umap = all_embeddings_umap[:train_size]\n",
    "val_embeddings_umap = all_embeddings_umap[train_size : train_size + val_size]\n",
    "test_embeddings_umap = all_embeddings_umap[train_size + val_size :]\n",
    "\n",
    "# Also getting the test set embeddings after TTA\n",
    "test_embeddings_with_TTA = embeddings_with_TTA(\n",
    "    best_model, test_loader, lr=learning_rate_TTA\n",
    ")\n",
    "\n",
    "test_embeddings_with_TTA_tsne = tsne.fit_transform(np.array(test_embeddings_with_TTA))\n",
    "test_embeddings_with_TTA_umap = umap_reducer.transform(\n",
    "    np.array(test_embeddings_with_TTA)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7d2cf6",
   "metadata": {},
   "source": [
    "### Analyzing the Solubility Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28a54a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the solubility values from each dataset\n",
    "train_solubility = train_loader.dataset.labels\n",
    "val_solubility = val_loader.dataset.labels\n",
    "test_solubility = test_loader.dataset.labels\n",
    "\n",
    "# Create a figure with two subplots side by side\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "# Define a consistent colormap for solubility\n",
    "cmap = plt.cm.viridis\n",
    "norm = plt.Normalize(\n",
    "    min(min(train_solubility), min(val_solubility), min(test_solubility)),\n",
    "    max(max(train_solubility), max(val_solubility), max(test_solubility)),\n",
    ")\n",
    "\n",
    "# Plot t-SNE projection on the left\n",
    "sc1 = ax1.scatter(\n",
    "    train_embeddings_tsne[:, 0],\n",
    "    train_embeddings_tsne[:, 1],\n",
    "    c=train_solubility,\n",
    "    cmap=cmap,\n",
    "    norm=norm,\n",
    "    s=1.5,\n",
    ")\n",
    "ax1.scatter(\n",
    "    val_embeddings_tsne[:, 0],\n",
    "    val_embeddings_tsne[:, 1],\n",
    "    c=val_solubility,\n",
    "    cmap=cmap,\n",
    "    norm=norm,\n",
    "    s=1.5,\n",
    ")\n",
    "ax1.scatter(\n",
    "    test_embeddings_tsne[:, 0],\n",
    "    test_embeddings_tsne[:, 1],\n",
    "    c=test_solubility,\n",
    "    cmap=cmap,\n",
    "    norm=norm,\n",
    "    s=1.5,\n",
    ")\n",
    "\n",
    "ax1.set_title(\"t-SNE Projection\")\n",
    "ax1.set_xlabel(\"t-SNE Component 1\")\n",
    "ax1.set_ylabel(\"t-SNE Component 2\")\n",
    "\n",
    "# Plot PCA projection on the right\n",
    "sc2 = ax2.scatter(\n",
    "    train_embeddings_umap[:, 0],\n",
    "    train_embeddings_umap[:, 1],\n",
    "    c=train_solubility,\n",
    "    cmap=cmap,\n",
    "    norm=norm,\n",
    "    s=1.5,\n",
    ")\n",
    "ax2.scatter(\n",
    "    val_embeddings_umap[:, 0],\n",
    "    val_embeddings_umap[:, 1],\n",
    "    c=val_solubility,\n",
    "    cmap=cmap,\n",
    "    norm=norm,\n",
    "    s=1.5,\n",
    ")\n",
    "ax2.scatter(\n",
    "    test_embeddings_umap[:, 0],\n",
    "    test_embeddings_umap[:, 1],\n",
    "    c=test_solubility,\n",
    "    cmap=cmap,\n",
    "    norm=norm,\n",
    "    s=1.5,\n",
    ")\n",
    "\n",
    "ax2.set_title(\"UMAP Projection\")\n",
    "ax2.set_xlabel(\"UMAP Component 1\")\n",
    "ax2.set_ylabel(\"UMAP Component 2\")\n",
    "\n",
    "# Dynamically adjust the limits for both plots\n",
    "x_min_train_tsne, x_max_train_tsne = np.percentile(train_embeddings_tsne[:, 0], [5, 95])\n",
    "y_min_train_tsne, y_max_train_tsne = np.percentile(train_embeddings_tsne[:, 1], [5, 95])\n",
    "x_min_test_tsne, x_max_test_tsne = np.percentile(test_embeddings_tsne[:, 0], [5, 95])\n",
    "y_min_test_tsne, y_max_test_tsne = np.percentile(test_embeddings_tsne[:, 1], [5, 95])\n",
    "\n",
    "x_max_tsne = max(x_max_train_tsne, x_max_test_tsne)\n",
    "x_min_tsne = min(x_min_train_tsne, x_min_test_tsne)\n",
    "y_max_tsne = max(y_max_train_tsne, y_max_test_tsne)\n",
    "y_min_tsne = min(y_min_train_tsne, y_min_test_tsne)\n",
    "\n",
    "ax1.set_xlim(x_min_tsne, x_max_tsne)\n",
    "ax1.set_ylim(y_min_tsne, y_max_tsne)\n",
    "\n",
    "x_min_train_umap, x_max_train_umap = np.percentile(train_embeddings_umap[:, 0], [5, 95])\n",
    "y_min_train_umap, y_max_train_umap = np.percentile(train_embeddings_umap[:, 1], [5, 95])\n",
    "x_min_test_umap, x_max_test_umap = np.percentile(test_embeddings_umap[:, 0], [5, 95])\n",
    "y_min_test_umap, y_max_test_umap = np.percentile(test_embeddings_umap[:, 1], [5, 95])\n",
    "\n",
    "x_max_umap = max(x_max_train_umap, x_max_test_umap)\n",
    "x_min_umap = min(x_min_train_umap, x_min_test_umap)\n",
    "y_max_umap = max(y_max_train_umap, y_max_test_umap)\n",
    "y_min_umap = min(y_min_train_umap, y_min_test_umap)\n",
    "\n",
    "ax2.set_xlim(x_min_umap, x_max_umap)\n",
    "ax2.set_ylim(y_min_umap, y_max_umap)\n",
    "\n",
    "fig.tight_layout()\n",
    "cbar = fig.colorbar(sc2, ax=[ax2], aspect=50, pad=0.015)\n",
    "cbar.set_label(\"Solubility\")\n",
    "\n",
    "if save_plots:\n",
    "    plt.savefig(\"figures/solubility.jpg\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.savefig(\"figures/solubility.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbbea10",
   "metadata": {},
   "source": [
    "### Analyzing the Class Distribution and Effect of TTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a43d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 2D embeddings using both t-SNE and PCA for the plot\n",
    "train_embeddings_2d = train_embeddings_umap\n",
    "val_embeddings_2d = val_embeddings_umap\n",
    "test_embeddings_2d = test_embeddings_umap\n",
    "test_embeddings_with_TTA_2d = test_embeddings_with_TTA_umap\n",
    "\n",
    "# Create a figure with two subplots side by side\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "# t-SNE plot on the left\n",
    "ax1.scatter(\n",
    "    train_embeddings_tsne[:, 0],\n",
    "    train_embeddings_tsne[:, 1],\n",
    "    label=\"Train Set\",\n",
    "    s=1.5,\n",
    "    color=\"blue\",\n",
    ")\n",
    "ax1.scatter(\n",
    "    val_embeddings_tsne[:, 0],\n",
    "    val_embeddings_tsne[:, 1],\n",
    "    label=\"Validation Set\",\n",
    "    s=1.5,\n",
    "    color=\"orange\",\n",
    ")\n",
    "ax1.scatter(\n",
    "    test_embeddings_tsne[:, 0],\n",
    "    test_embeddings_tsne[:, 1],\n",
    "    label=\"Test Set\",\n",
    "    s=1.5,\n",
    "    color=\"green\",\n",
    ")\n",
    "ax1.scatter(\n",
    "    test_embeddings_with_TTA_tsne[:, 0],\n",
    "    test_embeddings_with_TTA_tsne[:, 1],\n",
    "    label=\"Test Set with TTA\",\n",
    "    s=1.5,\n",
    "    color=\"red\",\n",
    ")\n",
    "\n",
    "# Calculate centroids\n",
    "train_centroid_tsne = np.mean(train_embeddings_tsne, axis=0)\n",
    "val_centroid_tsne = np.mean(val_embeddings_tsne, axis=0)\n",
    "test_centroid_tsne = np.mean(test_embeddings_tsne, axis=0)\n",
    "test_tta_centroid_tsne = np.mean(test_embeddings_with_TTA_tsne, axis=0)\n",
    "\n",
    "\n",
    "ax1.set_title(\"t-SNE Projection\")\n",
    "ax1.set_xlabel(\"t-SNE Component 1\")\n",
    "ax1.set_ylabel(\"t-SNE Component 2\")\n",
    "ax1.set_xlim(x_min_tsne, x_max_tsne)\n",
    "ax1.set_ylim(y_min_tsne, y_max_tsne)\n",
    "fig.legend(bbox_to_anchor=(0.75, -0.05), loc=\"lower right\", ncol=4)\n",
    "\n",
    "# PCA plot on the right\n",
    "ax2.scatter(\n",
    "    train_embeddings_2d[:, 0],\n",
    "    train_embeddings_2d[:, 1],\n",
    "    label=\"Train Set\",\n",
    "    s=1.5,\n",
    "    color=\"blue\",\n",
    ")\n",
    "ax2.scatter(\n",
    "    val_embeddings_2d[:, 0],\n",
    "    val_embeddings_2d[:, 1],\n",
    "    label=\"Validation Set\",\n",
    "    s=1.5,\n",
    "    color=\"orange\",\n",
    ")\n",
    "ax2.scatter(\n",
    "    test_embeddings_2d[:, 0],\n",
    "    test_embeddings_2d[:, 1],\n",
    "    label=\"Test Set\",\n",
    "    s=1.5,\n",
    "    color=\"green\",\n",
    ")\n",
    "ax2.scatter(\n",
    "    test_embeddings_with_TTA_2d[:, 0],\n",
    "    test_embeddings_with_TTA_2d[:, 1],\n",
    "    label=\"Test Set with TTA\",\n",
    "    s=1.5,\n",
    "    color=\"red\",\n",
    ")\n",
    "\n",
    "# Calculate centroids with median instead of mean since we have outliers\n",
    "train_centroid = np.median(train_embeddings_2d, axis=0)\n",
    "val_centroid = np.median(val_embeddings_2d, axis=0)\n",
    "test_centroid = np.median(test_embeddings_2d, axis=0)\n",
    "test_tta_centroid = np.median(test_embeddings_with_TTA_2d, axis=0)\n",
    "\n",
    "ax2.set_title(\"UMAP Projection\")\n",
    "ax2.set_xlabel(\"UMAP Component 1\")\n",
    "ax2.set_ylabel(\"UMAP Component 2\")\n",
    "ax2.set_xlim(x_min_umap, x_max_umap)\n",
    "ax2.set_ylim(y_min_umap, y_max_umap)\n",
    "\n",
    "plt.tight_layout()\n",
    "if save_plots:\n",
    "    plt.savefig(\"figures/sets.jpg\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.savefig(\"figures/sets.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc472a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with two subplots side by side\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "# t-SNE plot on the left\n",
    "ax1.scatter(\n",
    "    test_embeddings_tsne[:, 0],\n",
    "    test_embeddings_tsne[:, 1],\n",
    "    alpha=0.5,\n",
    "    label=\"Test Set without TTA\",\n",
    "    s=1.5,\n",
    "    color=\"green\",\n",
    ")\n",
    "ax1.scatter(\n",
    "    test_embeddings_with_TTA_tsne[:, 0],\n",
    "    test_embeddings_with_TTA_tsne[:, 1],\n",
    "    alpha=0.5,\n",
    "    label=\"Test Set with TTA\",\n",
    "    s=1.5,\n",
    "    color=\"red\",\n",
    ")\n",
    "\n",
    "# Plot centroids for t-SNE\n",
    "ax1.scatter(\n",
    "    train_centroid_tsne[0],\n",
    "    train_centroid_tsne[1],\n",
    "    s=250,\n",
    "    c=\"blue\",\n",
    "    marker=\"P\",\n",
    "    edgecolors=\"black\",\n",
    "    linewidths=1,\n",
    "    label=\"Train Centroid\",\n",
    ")\n",
    "ax1.scatter(\n",
    "    val_centroid_tsne[0],\n",
    "    val_centroid_tsne[1],\n",
    "    s=250,\n",
    "    c=\"orange\",\n",
    "    marker=\"X\",\n",
    "    edgecolors=\"black\",\n",
    "    linewidths=1,\n",
    "    label=\"Val Centroid\",\n",
    ")\n",
    "ax1.scatter(\n",
    "    test_centroid_tsne[0],\n",
    "    test_centroid_tsne[1],\n",
    "    s=250,\n",
    "    c=\"green\",\n",
    "    marker=\"P\",\n",
    "    edgecolors=\"black\",\n",
    "    linewidths=1,\n",
    "    label=\"Test Centroid\",\n",
    ")\n",
    "ax1.scatter(\n",
    "    test_tta_centroid_tsne[0],\n",
    "    test_tta_centroid_tsne[1],\n",
    "    s=250,\n",
    "    c=\"red\",\n",
    "    marker=\"X\",\n",
    "    edgecolors=\"black\",\n",
    "    linewidths=1,\n",
    "    label=\"Test Centroid with TTA\",\n",
    ")\n",
    "\n",
    "ax1.set_title(\"t-SNE Projection\")\n",
    "ax1.set_xlabel(\"t-SNE Component 1\")\n",
    "ax1.set_ylabel(\"t-SNE Component 2\")\n",
    "\n",
    "# Zooming in a bit further to see the centroids better\n",
    "x_min_train_tsne, x_max_train_tsne = np.percentile(\n",
    "    train_embeddings_tsne[:, 0], [20, 80]\n",
    ")\n",
    "y_min_train_tsne, y_max_train_tsne = np.percentile(\n",
    "    train_embeddings_tsne[:, 1], [20, 80]\n",
    ")\n",
    "x_min_test_tsne, x_max_test_tsne = np.percentile(test_embeddings_tsne[:, 0], [20, 80])\n",
    "y_min_test_tsne, y_max_test_tsne = np.percentile(test_embeddings_tsne[:, 1], [20, 80])\n",
    "\n",
    "x_max_tsne = max(x_max_train_tsne, x_max_test_tsne)\n",
    "x_min_tsne = min(x_min_train_tsne, x_min_test_tsne)\n",
    "y_max_tsne = max(y_max_train_tsne, y_max_test_tsne)\n",
    "y_min_tsne = min(y_min_train_tsne, y_min_test_tsne)\n",
    "\n",
    "ax1.set_xlim(x_min_tsne, x_max_tsne)\n",
    "ax1.set_ylim(y_min_tsne, y_max_tsne)\n",
    "fig.legend(bbox_to_anchor=(0.75, -0.1), loc=\"lower right\", ncol=3)\n",
    "\n",
    "# PCA plot on the right\n",
    "ax2.scatter(\n",
    "    test_embeddings_2d[:, 0],\n",
    "    test_embeddings_2d[:, 1],\n",
    "    alpha=0.5,\n",
    "    label=\"Test Set without TTA\",\n",
    "    s=1.5,\n",
    "    color=\"green\",\n",
    ")\n",
    "ax2.scatter(\n",
    "    test_embeddings_with_TTA_2d[:, 0],\n",
    "    test_embeddings_with_TTA_2d[:, 1],\n",
    "    alpha=0.5,\n",
    "    label=\"Test Set with TTA\",\n",
    "    s=1.5,\n",
    "    color=\"red\",\n",
    ")\n",
    "\n",
    "# Plot centroids for PCA\n",
    "ax2.scatter(\n",
    "    train_centroid[0],\n",
    "    train_centroid[1],\n",
    "    s=250,\n",
    "    c=\"blue\",\n",
    "    marker=\"P\",\n",
    "    edgecolors=\"black\",\n",
    "    linewidths=1,\n",
    "    label=\"Train Centroid\",\n",
    "    alpha=0.9,\n",
    ")\n",
    "ax2.scatter(\n",
    "    val_centroid[0],\n",
    "    val_centroid[1],\n",
    "    s=250,\n",
    "    c=\"orange\",\n",
    "    marker=\"X\",\n",
    "    edgecolors=\"black\",\n",
    "    linewidths=1,\n",
    "    label=\"Val Centroid\",\n",
    "    alpha=0.9,\n",
    ")\n",
    "ax2.scatter(\n",
    "    test_centroid[0],\n",
    "    test_centroid[1],\n",
    "    s=250,\n",
    "    c=\"green\",\n",
    "    marker=\"P\",\n",
    "    edgecolors=\"black\",\n",
    "    linewidths=1,\n",
    "    label=\"Test Centroid\",\n",
    "    alpha=0.9,\n",
    ")\n",
    "ax2.scatter(\n",
    "    test_tta_centroid[0],\n",
    "    test_tta_centroid[1],\n",
    "    s=250,\n",
    "    c=\"red\",\n",
    "    marker=\"X\",\n",
    "    edgecolors=\"black\",\n",
    "    linewidths=1,\n",
    "    label=\"Test Centroid with TTA\",\n",
    "    alpha=0.9,\n",
    ")\n",
    "\n",
    "ax2.set_title(\"UMAP Projection\")\n",
    "ax2.set_xlabel(\"UMAP Component 1\")\n",
    "ax2.set_ylabel(\"UMAP Component 2\")\n",
    "\n",
    "# Zooming in a bit further to see the centroids better\n",
    "x_min_train_umap, x_max_train_umap = np.percentile(\n",
    "    train_embeddings_umap[:, 0], [20, 80]\n",
    ")\n",
    "y_min_train_umap, y_max_train_umap = np.percentile(\n",
    "    train_embeddings_umap[:, 1], [20, 80]\n",
    ")\n",
    "x_min_test_umap, x_max_test_umap = np.percentile(test_embeddings_umap[:, 0], [20, 80])\n",
    "y_min_test_umap, y_max_test_umap = np.percentile(test_embeddings_umap[:, 1], [20, 80])\n",
    "\n",
    "x_max_umap = max(x_max_train_umap, x_max_test_umap)\n",
    "x_min_umap = min(x_min_train_umap, x_min_test_umap)\n",
    "y_max_umap = max(y_max_train_umap, y_max_test_umap)\n",
    "y_min_umap = min(y_min_train_umap, y_min_test_umap)\n",
    "\n",
    "ax2.set_xlim(x_min_umap, x_max_umap)\n",
    "ax2.set_ylim(y_min_umap, y_max_umap)\n",
    "\n",
    "plt.tight_layout()\n",
    "if save_plots:\n",
    "    plt.savefig(\"figures/sets_TTA.jpg\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.savefig(\"figures/sets_TTA.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
