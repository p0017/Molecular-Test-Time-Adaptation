{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee7ec02f-bc61-4c89-987d-b3025b2a474f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from copy import deepcopy\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch_geometric as tg\n",
    "from torch_geometric.data import Dataset, Batch\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.nn import global_add_pool\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, root_mean_squared_error\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb419790-ae4f-478b-8ecf-843e1aa4375a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding_unk(value, choices: list) -> list:\n",
    "    # One hot encoding with unknown value handling\n",
    "    # If the value is in choices, it puts a 1 at the corresponding index\n",
    "    # Otherwise, it puts a 1 at the last index (unknown)\n",
    "    encoding = [0] * (len(choices) + 1)\n",
    "    index = choices.index(value) if value in choices else -1\n",
    "    encoding[index] = 1\n",
    "    return encoding\n",
    "\n",
    "\n",
    "def get_atom_features(atom) -> list:\n",
    "    # Returns a feature list for the atom\n",
    "    # Concatenates the one-hot encodings into a single list\n",
    "    features = [\n",
    "        one_hot_encoding_unk(atom.GetSymbol(), ['B','Be','Br','C','Cl','F','I','N','Nb','O','P','S','Se','Si','V','W']),\n",
    "        one_hot_encoding_unk(atom.GetTotalDegree(), [0, 1, 2, 3, 4, 5]),\n",
    "        one_hot_encoding_unk(atom.GetFormalCharge(), [-1, -2, 1, 2, 0]),\n",
    "        one_hot_encoding_unk(int(atom.GetTotalNumHs()), [0, 1, 2, 3, 4]),\n",
    "        one_hot_encoding_unk(int(atom.GetHybridization()),[\n",
    "                                                        Chem.rdchem.HybridizationType.SP,\n",
    "                                                        Chem.rdchem.HybridizationType.SP2,\n",
    "                                                        Chem.rdchem.HybridizationType.SP3,\n",
    "                                                        Chem.rdchem.HybridizationType.SP3D,\n",
    "                                                        Chem.rdchem.HybridizationType.SP3D2\n",
    "                                                        ]),\n",
    "        [1 if atom.GetIsAromatic() else 0],\n",
    "        [atom.GetMass() * 0.01]\n",
    "    ]\n",
    "    return sum(features, []) # Flatten the list into a single list\n",
    "\n",
    "\n",
    "def get_bond_features(bond) -> list:\n",
    "    # Returns a one-hot encoded feature list for the bond\n",
    "    bond_fdim = 7\n",
    "\n",
    "    if bond is None:\n",
    "        fbond = [1] + [0] * (bond_fdim - 1)\n",
    "    else:\n",
    "        bt = bond.GetBondType()\n",
    "        fbond = [\n",
    "            0,  # Zeroth index indicates if bond is None\n",
    "            bt == Chem.rdchem.BondType.SINGLE,\n",
    "            bt == Chem.rdchem.BondType.DOUBLE,\n",
    "            bt == Chem.rdchem.BondType.TRIPLE,\n",
    "            bt == Chem.rdchem.BondType.AROMATIC,\n",
    "            (bond.GetIsConjugated() if bt is not None else 0),\n",
    "            (bond.IsInRing() if bt is not None else 0)\n",
    "        ]\n",
    "    return fbond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4af251a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MolGraph:\n",
    "    # Returns a custom molecular graph for a given SMILES string\n",
    "    # Contains atom, bond features and node connectivity\n",
    "    def __init__(self, smiles: str):\n",
    "        self.smiles = smiles\n",
    "        self.f_atoms = []\n",
    "        self.f_bonds = []\n",
    "        self.edge_index = []\n",
    "\n",
    "        mol = Chem.MolFromSmiles(self.smiles)\n",
    "        n_atoms=mol.GetNumAtoms()\n",
    "\n",
    "        for atom_1 in range(n_atoms):\n",
    "            self.f_atoms.append(get_atom_features(mol.GetAtomWithIdx(atom_1)))\n",
    "\n",
    "            for atom_2 in range(atom_1 + 1, n_atoms):\n",
    "                bond = mol.GetBondBetweenAtoms(atom_1, atom_2)\n",
    "                if bond is None:\n",
    "                    continue\n",
    "                f_bond = get_bond_features(bond)\n",
    "                self.f_bonds.append(f_bond)\n",
    "                self.f_bonds.append(f_bond) # Bond features are added twice for both directions\n",
    "                self.edge_index.extend([(atom_1, atom_2), (atom_2, atom_1)]) # Edge index list with tuples of connected nodes instead of adjacency matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6313a147",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChemDataset(Dataset):\n",
    "    def __init__(self, smiles: str, labels, noise: float=0.25, precompute: bool=True):\n",
    "        super(ChemDataset, self).__init__()\n",
    "        self.smiles = smiles\n",
    "        self.labels = labels\n",
    "        self.cache = {}\n",
    "        self.noise = noise\n",
    "        self.precompute = precompute\n",
    "\n",
    "        # Precomputing the dataset so the get method is faster, and the GPU doesn't have to wait for the CPU\n",
    "        if precompute:\n",
    "            print(f\"Precomputing data...\")\n",
    "            with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "                futures = [\n",
    "                    executor.submit(self.process_key , idx)\n",
    "                    for idx in range(len(self.smiles))\n",
    "                ]\n",
    "\n",
    "                for future in as_completed(futures):\n",
    "                    future.result()\n",
    "\n",
    "            print(f\"Precomputation finished. {len(self.cache)} molecules cached.\")\n",
    "\n",
    "    def process_key(self, key):\n",
    "        smiles = self.smiles[key]\n",
    "        if smiles in self.cache.keys():\n",
    "            mol = self.cache[smiles]\n",
    "        else:\n",
    "            molgraph = MolGraph(smiles)\n",
    "            mol = self.molgraph2data(molgraph, key)\n",
    "            self.cache[smiles] = mol\n",
    "        return mol\n",
    "\n",
    "    def molgraph2data(self, molgraph, key):\n",
    "        data = tg.data.Data()\n",
    "\n",
    "        # Coverting all features and labels to tensors\n",
    "        # And adding it to the data object\n",
    "        data.x = torch.tensor(molgraph.f_atoms, dtype=torch.float)\n",
    "        data.edge_index = torch.tensor(molgraph.edge_index, dtype=torch.long).t().contiguous()\n",
    "        data.edge_attr = torch.tensor(molgraph.f_bonds, dtype=torch.float)\n",
    "        data.y = torch.tensor([self.labels[key]], dtype=torch.float)\n",
    "        data.smiles = self.smiles[key]\n",
    "\n",
    "        if self.noise > 0:\n",
    "            x_noisy = deepcopy(data.x) + torch.randn_like(data.x) * self.noise\n",
    "            data.x_noisy = x_noisy\n",
    "\n",
    "        return data\n",
    "\n",
    "    def get(self,key):\n",
    "        return self.process_key(key)\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        return self.process_key(key)\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.smiles)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65270799",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_loader(data_df: pd.DataFrame, smi_column: str, target_column: str, shuffle: bool=True, batch_size: int=64):    \n",
    "    smiles = data_df[smi_column].values\n",
    "    labels = data_df[target_column].values.astype(np.float32)  \n",
    "    \n",
    "    dataset = ChemDataset(smiles, labels, precompute=True)\n",
    "    loader = DataLoader(dataset=dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=shuffle,\n",
    "                            pin_memory=True\n",
    "                       )\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecfaef5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DMPNNConv(MessagePassing): # Extending the MessagePassing class from PyG\n",
    "    def __init__(self, hidden_size: int):\n",
    "        super(DMPNNConv, self).__init__(aggr='add') # Sum aggregation function\n",
    "        self.lin = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, edge_index, edge_attr):\n",
    "        row, _ = edge_index\n",
    "        # Since each edge is bidirectional, we do two message passings, one for each direction\n",
    "        aggregated_message = self.propagate(edge_index, x=None, edge_attr=edge_attr)\n",
    "        reversed_message = torch.flip(edge_attr.view(edge_attr.size(0) // 2, 2, -1), dims=[1]).view(edge_attr.size(0), -1)\n",
    "\n",
    "        return aggregated_message, self.lin(aggregated_message[row] - reversed_message)\n",
    "\n",
    "    def message(self, edge_attr):\n",
    "        return edge_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ce58a11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNEncoder(nn.Module):\n",
    "    def __init__(self, num_node_features: int, num_edge_features: int, hidden_size: int, mode: str, depth: int, dropout: float=0.2):\n",
    "        super().__init__()\n",
    "        self.depth = depth\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout = dropout\n",
    "        self.mode = mode\n",
    "\n",
    "        # Encoder layers\n",
    "        self.edge_init = nn.Linear(num_node_features + num_edge_features, hidden_size)\n",
    "        self.convs = nn.ModuleList([DMPNNConv(hidden_size) for _ in range(depth)])\n",
    "        self.edge_to_node = nn.Linear(num_node_features + hidden_size, hidden_size)\n",
    "        self.pool = global_add_pool  # Not learnable\n",
    "\n",
    "    def forward(self, data):\n",
    "        edge_index, edge_attr, batch = data.edge_index, data.edge_attr, data.batch\n",
    "        \n",
    "        if self.mode == 'denoise':\n",
    "            x = data.x_noisy\n",
    "        elif self.mode == 'predict':\n",
    "            x = data.x\n",
    "        else:\n",
    "            raise ValueError(\"Invalid mode. Choose 'denoise' or 'predict'.\")\n",
    "\n",
    "        # Edge initialization\n",
    "        row, _ = edge_index\n",
    "        h_0 = F.relu(self.edge_init(torch.cat([x[row], edge_attr], dim=1)))\n",
    "        h = h_0\n",
    "\n",
    "        # DMPNN Conv layers\n",
    "        for layer in self.convs:\n",
    "            _, h = layer(edge_index, h)\n",
    "            h += h_0\n",
    "            h = F.dropout(F.relu(h), self.dropout, training=self.training)\n",
    "\n",
    "        # Edge to node aggregation\n",
    "        # Re-using the last layer's results for s\n",
    "        s, _ = self.convs[-1](edge_index, h)\n",
    "        \n",
    "        # Due to a recurring error which I can't figure out, we add a check here\n",
    "        # to ensure that the sizes of s and x match\n",
    "        # This is a workaround and should be fixed in the future\n",
    "        # Luckily, this issue only occurs for same-sample batches, which are needed for TTA\n",
    "\n",
    "        # Pad/truncate s to match x's size\n",
    "        if s.shape[0] != x.shape[0]:\n",
    "            # Create tensor with same length as x (regardless of connectivity)\n",
    "            s_fixed = torch.zeros(x.shape[0], self.hidden_size, device=s.device)\n",
    "            # Only use the connected nodes we have (first min(s.shape[0], x.shape[0]))\n",
    "            min_len = min(s.shape[0], x.shape[0]) \n",
    "            s_fixed[:min_len] = s[:min_len]\n",
    "            s = s_fixed\n",
    "\n",
    "        q = torch.cat([x, s], dim=1)\n",
    "        h = F.relu(self.edge_to_node(q))\n",
    "\n",
    "        # Global pooling for the final node embeddings\n",
    "        embedding = self.pool(h, batch)\n",
    "        \n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "12dcc5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNDecoder(nn.Module):\n",
    "    # Decoder for self-supervised denoising task\n",
    "    # Only reconstructing the node features from the embeddings\n",
    "    def __init__(self, hidden_size: int, num_node_features: int):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.lin2 = nn.Linear(hidden_size, num_node_features)\n",
    "\n",
    "    def forward(self, graph_embedding, batch):\n",
    "        # The embedding size is (batch_size, hidden_size)\n",
    "        # We need to expand it per node. First, get how many nodes per graph:\n",
    "        batch_size = graph_embedding.size(0)\n",
    "        node_counts = torch.bincount(batch)  # number of nodes in each graph\n",
    "\n",
    "        # Expand each graph embedding \"node_counts[g]\" times\n",
    "        expanded = []\n",
    "        for g in range(batch_size):\n",
    "            repeated = graph_embedding[g].unsqueeze(0).repeat(node_counts[g], 1)  # shape = (num_nodes_in_g, hidden_size)\n",
    "            expanded.append(repeated)\n",
    "\n",
    "        # Concatenate along node dimension\n",
    "        expanded = torch.cat(expanded, dim=0)  # total_nodes x hidden_size\n",
    "\n",
    "        # Decode the node features\n",
    "        x_hat = F.relu(self.lin1(expanded))\n",
    "        x_hat = self.lin2(x_hat)  # shape = (total_nodes, num_node_features)\n",
    "\n",
    "        return x_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a1dba7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNHead(nn.Module):\n",
    "    def __init__(self, hidden_size: int, dropout: float=0.2):\n",
    "        super().__init__()\n",
    "        self.ffn1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.ffn2 = nn.Linear(hidden_size, 1)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, graph_embedding):\n",
    "        x = F.relu(self.ffn1(graph_embedding))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        return self.ffn2(x).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ed6dc19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(nn.Module):\n",
    "    def __init__(self, num_node_features: int, num_edge_features: int, hidden_size: int=300, depth: int=3, mode: str='denoise', dropout: float=0.02):\n",
    "        super().__init__()\n",
    "        self.encoder = GNNEncoder(num_node_features, num_edge_features, hidden_size=hidden_size, mode=mode, depth=depth, dropout=dropout)\n",
    "        self.head = GNNHead(hidden_size=hidden_size, dropout=dropout)\n",
    "        self.decoder = GNNDecoder(hidden_size=hidden_size, num_node_features=num_node_features)\n",
    "\n",
    "    def set_mode(self, mode: str):\n",
    "        # Update the mode in the encoder\n",
    "        # So the encoder knows if it needs to read noisy or noise-free data\n",
    "        self.encoder.mode = mode\n",
    "\n",
    "    def forward(self, data):\n",
    "        graph_embedding = self.encoder(data)\n",
    "\n",
    "        if self.encoder.mode == 'predict':\n",
    "            prediction = self.head(graph_embedding)\n",
    "            return prediction\n",
    "        \n",
    "        elif self.encoder.mode == 'denoise':\n",
    "            node_features = self.decoder(graph_embedding, data.batch)\n",
    "            return node_features\n",
    "    \n",
    "        else:\n",
    "            raise ValueError(\"Invalid mode. Choose 'predict' or 'denoise'.\")\n",
    "        \n",
    "    def encode(self, data):\n",
    "        return self.encoder(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dd052185",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Standardizer:\n",
    "    def __init__(self, mean: float, std: float):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, x, rev: bool=False):\n",
    "        if rev:\n",
    "            return (x * self.std) + self.mean\n",
    "        return (x - self.mean) / self.std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "62ea2a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "83402641",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, loss, mode: str, stdzer: Standardizer=None):\n",
    "    model = model.to(device)\n",
    "    if mode == 'denoise':\n",
    "        model.set_mode('denoise')\n",
    "        model.train()\n",
    "        loss_all = 0\n",
    "\n",
    "        for data in loader:\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            out = model(data)\n",
    "            result = loss(out, data.x)\n",
    "            result.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            loss_all += loss(out, data.x)\n",
    "\n",
    "        return math.sqrt(loss_all / len(loader.dataset))\n",
    "\n",
    "    elif mode == 'predict':\n",
    "        model.set_mode('predict')\n",
    "        model.train()\n",
    "        loss_all = 0\n",
    "\n",
    "        for data in loader:\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            out = model(data)\n",
    "            result = loss(out, stdzer(data.y))\n",
    "            result.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            loss_all += loss(stdzer(out, rev=True), data.y)\n",
    "\n",
    "        return math.sqrt(loss_all / len(loader.dataset))\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Invalid mode. Choose 'denoise' or 'predict'.\")\n",
    "\n",
    "\n",
    "def pred(model, loader, mode: str, stdzer: Standardizer=None):\n",
    "    model = model.to(device)\n",
    "    if mode == 'denoise':\n",
    "        model.set_mode('denoise')\n",
    "        model.eval()\n",
    "\n",
    "        preds = []\n",
    "        with torch.no_grad():\n",
    "            for data in loader:\n",
    "                data = data.to(device)\n",
    "                out = model(data)\n",
    "                preds.extend(out.cpu().detach().flatten().tolist())\n",
    "                \n",
    "        return preds\n",
    "\n",
    "    elif mode == 'predict':\n",
    "        model.set_mode('predict')\n",
    "        model.eval()\n",
    "\n",
    "        preds = []\n",
    "        with torch.no_grad():\n",
    "            for data in loader:\n",
    "                data = data.to(device)\n",
    "                out = model(data)\n",
    "                pred = stdzer(out, rev=True)\n",
    "                preds.extend(pred.cpu().detach().tolist())\n",
    "\n",
    "        return preds\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Invalid mode. Choose 'denoise' or 'predict'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e699e0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing [1263, 1444, 3605, 3702] due to single atoms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20:42:51] Explicit valence for atom # 1 P, 6, is greater than permitted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing [667] due to Nonetypes\n",
      "Removing [471, 503, 589, 591, 592, 593, 594, 610, 613, 641, 643, 647, 649, 652, 653, 654, 656, 658, 676, 681, 693, 744, 759, 763, 769, 773, 777, 807, 809, 811, 813, 869, 902, 969, 998] due to single atoms\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "data_df = pd.read_csv(\"AqSolDBc.csv\")\n",
    "# Drop single atoms    \n",
    "idx_single = [i for i,s in enumerate(data_df['SmilesCurated']) if Chem.MolFromSmiles(s).GetNumAtoms()==1 or '.' in s]\n",
    "data_df = data_df.drop(idx_single)\n",
    "if len(idx_single) > 0:\n",
    "    print(f\"Removing {idx_single} due to single atoms\")\n",
    "\n",
    "test_df = pd.read_csv(\"OChemUnseen.csv\")\n",
    "# Drop single atoms\n",
    "idx_nonetype = [i for i,s in enumerate(test_df['SMILES']) if Chem.MolFromSmiles(s) is None] # Got an error for a SMILES which was None\n",
    "test_df = test_df.drop(idx_nonetype)\n",
    "if len(idx_nonetype) > 0:\n",
    "    print(f\"Removing {idx_nonetype} due to Nonetypes\")\n",
    "\n",
    "idx_single = [i for i,s in enumerate(test_df['SMILES']) if Chem.MolFromSmiles(s).GetNumAtoms()==1 or '.' in s]\n",
    "test_df = test_df.drop(idx_single)\n",
    "if len(idx_single) > 0:\n",
    "    print(f\"Removing {idx_single} due to single atoms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144f5af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precomputing data...\n",
      "Precomputation finished. 7238 molecules cached.\n",
      "Precomputing data...\n",
      "Precomputation finished. 805 molecules cached.\n",
      "Precomputing data...\n",
      "Precomputation finished. 2215 molecules cached.\n",
      "Train size: 7238, Val size: 805, Test size: 2215\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "\n",
    "train_df, val_df = train_test_split(data_df, test_size=0.1, random_state=0)\n",
    "train_loader = construct_loader(train_df, 'SmilesCurated', 'ExperimentalLogS', shuffle=True, batch_size=batch_size)\n",
    "val_loader = construct_loader(val_df, 'SmilesCurated', 'ExperimentalLogS', shuffle=False, batch_size=batch_size)\n",
    "test_loader = construct_loader(test_df, 'SMILES', 'LogS', shuffle=False, batch_size=batch_size)\n",
    "print(f\"Train size: {len(train_loader.dataset)}, Val size: {len(val_loader.dataset)}, Test size: {len(test_loader.dataset)}\")\n",
    "\n",
    "# Normalizer for the labels\n",
    "mean = np.mean(train_loader.dataset.labels)\n",
    "std = np.std(train_loader.dataset.labels)\n",
    "stdzer = Standardizer(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "014f216f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " GNN(\n",
      "  (encoder): GNNEncoder(\n",
      "    (edge_init): Linear(in_features=51, out_features=300, bias=True)\n",
      "    (convs): ModuleList(\n",
      "      (0-2): 3 x DMPNNConv()\n",
      "    )\n",
      "    (edge_to_node): Linear(in_features=344, out_features=300, bias=True)\n",
      "  )\n",
      "  (head): GNNHead(\n",
      "    (ffn1): Linear(in_features=300, out_features=300, bias=True)\n",
      "    (ffn2): Linear(in_features=300, out_features=1, bias=True)\n",
      "  )\n",
      "  (decoder): GNNDecoder(\n",
      "    (lin1): Linear(in_features=300, out_features=300, bias=True)\n",
      "    (lin2): Linear(in_features=300, out_features=44, bias=True)\n",
      "  )\n",
      ") \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = GNN(train_loader.dataset.num_node_features, train_loader.dataset.num_edge_features).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss = nn.MSELoss(reduction='sum').to(device)\n",
    "print('\\n', model, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f7bbd0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Train RMSE: 8.67   Val RMSE: 0.257\n",
      "Epoch 1  Train RMSE: 6.53   Val RMSE: 0.228\n",
      "Epoch 2  Train RMSE: 6.15   Val RMSE: 0.222\n",
      "Epoch 3  Train RMSE: 6.01   Val RMSE: 0.22\n",
      "Epoch 4  Train RMSE: 5.97   Val RMSE: 0.218\n"
     ]
    }
   ],
   "source": [
    "# Unfreeze the encoder\n",
    "for param in model.encoder.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Unfreeze the decoder\n",
    "for param in model.decoder.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Freeze the prediction head\n",
    "for param in model.head.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "best_denoise_model = deepcopy(model).to(device)\n",
    "best_denoise_val_loss = 1000000\n",
    "\n",
    "for epoch in range(0, 5):\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, loss, mode='denoise')\n",
    "    preds = pred(model, val_loader, mode='denoise')\n",
    "    node_feature_targets = [feature for batch in val_loader for feature in batch.x.cpu().flatten().tolist()]\n",
    "    val_loss = root_mean_squared_error(preds, node_feature_targets)\n",
    "    print(f\"Epoch {epoch}  Train RMSE: {train_loss:.3g}   Val RMSE: {val_loss:.3g}\")\n",
    "\n",
    "    if val_loss < best_denoise_val_loss:\n",
    "        best_denoise_model = deepcopy(model).to(device)\n",
    "        best_denoise_val_loss = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bcdbf024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Train RMSE: 1.59   Val RMSE: 1.3\n",
      "Epoch 1  Train RMSE: 1.25   Val RMSE: 1.2\n",
      "Epoch 2  Train RMSE: 1.18   Val RMSE: 1.11\n",
      "Epoch 3  Train RMSE: 1.13   Val RMSE: 1.09\n",
      "Epoch 4  Train RMSE: 1.1   Val RMSE: 1.06\n",
      "Epoch 5  Train RMSE: 1.08   Val RMSE: 1.08\n",
      "Epoch 6  Train RMSE: 1.06   Val RMSE: 1.21\n",
      "Epoch 7  Train RMSE: 1.05   Val RMSE: 1.06\n",
      "Epoch 8  Train RMSE: 1.03   Val RMSE: 1.09\n",
      "Epoch 9  Train RMSE: 1.02   Val RMSE: 1.04\n",
      "Epoch 10  Train RMSE: 1.01   Val RMSE: 1.03\n",
      "Epoch 11  Train RMSE: 1   Val RMSE: 1\n",
      "Epoch 12  Train RMSE: 0.996   Val RMSE: 1.08\n",
      "Epoch 13  Train RMSE: 0.997   Val RMSE: 1.01\n",
      "Epoch 14  Train RMSE: 0.977   Val RMSE: 1.06\n",
      "Test RMSE: 1.34\n",
      "Test MAE: 0.956\n"
     ]
    }
   ],
   "source": [
    "# Freeze the encoder\n",
    "for param in model.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Freeze the decoder\n",
    "for param in model.decoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze the prediction head\n",
    "for param in model.head.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "best_pred_model = deepcopy(best_denoise_model).to(device)\n",
    "best_val_loss = 1000000\n",
    "\n",
    "for epoch in range(0, 5):\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, loss, mode='predict', stdzer=stdzer)\n",
    "    preds = pred(model, val_loader, mode='predict', stdzer=stdzer)\n",
    "    val_loss = root_mean_squared_error(preds, val_loader.dataset.labels)\n",
    "    print(f\"Epoch {epoch}  Train RMSE: {train_loss:.3g}   Val RMSE: {val_loss:.3g}\")\n",
    "    if val_loss < best_val_loss:\n",
    "        best_pred_model = deepcopy(model).to(device)\n",
    "        best_val_loss = val_loss\n",
    "\n",
    "preds = pred(best_pred_model, test_loader, mode='predict', stdzer=stdzer)  \n",
    "print(f\"Test RMSE: {root_mean_squared_error(preds, test_loader.dataset.labels):.3g}\")\n",
    "print(f\"Test MAE: {mean_absolute_error(preds, test_loader.dataset.labels):.3g}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e771b4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_with_TTA(model, loader, batch_size: int=64, lr: float=0.001, stdzer:Standardizer=None):\n",
    "    # Unfreeze the encoder\n",
    "    for param in model.encoder.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    # Unfreeze the decoder\n",
    "    for param in model.decoder.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    # Freeze the prediction head\n",
    "    for param in model.head.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    model = deepcopy(model).to(device)\n",
    "    model_before_step = deepcopy(model).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    preds = []\n",
    "\n",
    "    for data in loader:\n",
    "        \n",
    "        model.set_mode('denoise')\n",
    "        model.train()\n",
    "\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        result = loss(out, data.x)\n",
    "        result.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        model.set_mode('predict')\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            data = data.to(device)\n",
    "            out = model(data)\n",
    "            pred = stdzer(out, rev=True)\n",
    "            preds.extend(pred.cpu().detach().tolist())\n",
    "\n",
    "        model = deepcopy(model_before_step)\n",
    "        \n",
    "    return preds[::batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b4511315",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RepeatedDataset(Dataset):\n",
    "    def __init__(self, base_dataset, repeat: int=64):\n",
    "        self.base = base_dataset\n",
    "        self.repeat = repeat\n",
    "        self.base_len = len(base_dataset)\n",
    "\n",
    "    def __len__(self):\n",
    "        # Each original sample is repeated `repeat` times\n",
    "        return self.base_len * self.repeat\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample_idx = idx // self.repeat\n",
    "        return deepcopy(self.base[sample_idx])\n",
    "\n",
    "def construct_single_sample_loader(data_df, smi_column: str, target_column: str, shuffle: bool=True, batch_size: int=64):\n",
    "    smiles = data_df[smi_column].values\n",
    "    labels = data_df[target_column].values.astype(np.float32)\n",
    "\n",
    "    base_dataset = ChemDataset(smiles, labels, precompute=True)\n",
    "    # Each sample is repeated 64 times\n",
    "    repeated_dataset = RepeatedDataset(base_dataset, repeat=batch_size)\n",
    "\n",
    "    # Each batch is a single repeated sample of size 64\n",
    "    loader = DataLoader(\n",
    "        dataset=repeated_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2c8e1ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precomputing data...\n",
      "Precomputation finished. 2215 molecules cached.\n"
     ]
    }
   ],
   "source": [
    "test_loader_single = construct_single_sample_loader(test_df, 'SMILES', 'LogS', shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54c4abf",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RepeatedDataset' object has no attribute 'labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m preds_TTA \u001b[38;5;241m=\u001b[39m pred_with_TTA(best_pred_model, test_loader_single, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m, stdzer\u001b[38;5;241m=\u001b[39mstdzer)\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest RMSE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mroot_mean_squared_error(preds_TTA,\u001b[38;5;250m \u001b[39m\u001b[43mtest_loader_single\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabels\u001b[49m)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3g\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest MAE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_absolute_error(preds_TTA,\u001b[38;5;250m \u001b[39mtest_loader_single\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mlabels)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3g\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'RepeatedDataset' object has no attribute 'labels'"
     ]
    }
   ],
   "source": [
    "preds_TTA = pred_with_TTA(best_pred_model, test_loader_single, lr=0.001, stdzer=stdzer)\n",
    "print(f\"Test RMSE: {root_mean_squared_error(preds_TTA, test_loader.dataset.labels):.3g}\")\n",
    "print(f\"Test MAE: {mean_absolute_error(preds_TTA, test_loader.dataset.labels):.3g}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1d4f1840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 1.34\n",
      "Test MAE: 0.956\n"
     ]
    }
   ],
   "source": [
    "preds = pred(best_pred_model, test_loader, mode='predict', stdzer=stdzer)  \n",
    "print(f\"Test RMSE: {root_mean_squared_error(preds, test_loader.dataset.labels):.3g}\")\n",
    "print(f\"Test MAE: {mean_absolute_error(preds, test_loader.dataset.labels):.3g}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
