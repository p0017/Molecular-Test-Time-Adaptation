{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee7ec02f-bc61-4c89-987d-b3025b2a474f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from copy import deepcopy\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch_geometric as tg\n",
    "from torch_geometric.data import Dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.nn import global_add_pool\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, root_mean_squared_error\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb419790-ae4f-478b-8ecf-843e1aa4375a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding_unk(value, choices):\n",
    "    # One hot encoding with unknown value handling\n",
    "    # If the value is in choices, it puts a 1 at the corresponding index\n",
    "    # Otherwise, it puts a 1 at the last index (unknown)\n",
    "    encoding = [0] * (len(choices) + 1)\n",
    "    index = choices.index(value) if value in choices else -1\n",
    "    encoding[index] = 1\n",
    "    return encoding\n",
    "\n",
    "\n",
    "def get_atom_features(atom):\n",
    "    # Returns a feature list for the atom\n",
    "    # Concatenates the one-hot encodings into a single list\n",
    "    features = [\n",
    "        one_hot_encoding_unk(atom.GetSymbol(), ['B','Be','Br','C','Cl','F','I','N','Nb','O','P','S','Se','Si','V','W']),\n",
    "        one_hot_encoding_unk(atom.GetTotalDegree(), [0, 1, 2, 3, 4, 5]),\n",
    "        one_hot_encoding_unk(atom.GetFormalCharge(), [-1, -2, 1, 2, 0]),\n",
    "        one_hot_encoding_unk(int(atom.GetTotalNumHs()), [0, 1, 2, 3, 4]),\n",
    "        one_hot_encoding_unk(int(atom.GetHybridization()),[\n",
    "                                                        Chem.rdchem.HybridizationType.SP,\n",
    "                                                        Chem.rdchem.HybridizationType.SP2,\n",
    "                                                        Chem.rdchem.HybridizationType.SP3,\n",
    "                                                        Chem.rdchem.HybridizationType.SP3D,\n",
    "                                                        Chem.rdchem.HybridizationType.SP3D2\n",
    "                                                        ]),\n",
    "        [1 if atom.GetIsAromatic() else 0],\n",
    "        [atom.GetMass() * 0.01]\n",
    "    ]\n",
    "    return sum(features, []) # Flatten the list into a single list\n",
    "\n",
    "\n",
    "def get_bond_features(bond):\n",
    "    # Returns a one-hot encoded feature list for the bond\n",
    "    bond_fdim = 7\n",
    "\n",
    "    if bond is None:\n",
    "        fbond = [1] + [0] * (bond_fdim - 1)\n",
    "    else:\n",
    "        bt = bond.GetBondType()\n",
    "        fbond = [\n",
    "            0,  # Zeroth index indicates if bond is None\n",
    "            bt == Chem.rdchem.BondType.SINGLE,\n",
    "            bt == Chem.rdchem.BondType.DOUBLE,\n",
    "            bt == Chem.rdchem.BondType.TRIPLE,\n",
    "            bt == Chem.rdchem.BondType.AROMATIC,\n",
    "            (bond.GetIsConjugated() if bt is not None else 0),\n",
    "            (bond.IsInRing() if bt is not None else 0)\n",
    "        ]\n",
    "    return fbond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af251a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MolGraph:\n",
    "    # Returns a custom molecular graph for a given SMILES string\n",
    "    # Contains atom, bond features and node connectivity\n",
    "    def __init__(self, smiles):\n",
    "        self.smiles = smiles\n",
    "        self.f_atoms = []\n",
    "        self.f_bonds = []\n",
    "        self.edge_index = []\n",
    "\n",
    "        mol = Chem.MolFromSmiles(self.smiles)\n",
    "        n_atoms=mol.GetNumAtoms()\n",
    "\n",
    "        for atom_1 in range(n_atoms):\n",
    "            self.f_atoms.append(get_atom_features(mol.GetAtomWithIdx(atom_1)))\n",
    "\n",
    "            for atom_2 in range(atom_1 + 1, n_atoms):\n",
    "                bond = mol.GetBondBetweenAtoms(atom_1, atom_2)\n",
    "                if bond is None:\n",
    "                    continue\n",
    "                f_bond = get_bond_features(bond)\n",
    "                self.f_bonds.append(f_bond)\n",
    "                self.f_bonds.append(f_bond) # Bond features are added twice for both directions\n",
    "                self.edge_index.extend([(atom_1, atom_2), (atom_2, atom_1)]) # Edge index list with tuples of connected nodes instead of adjacency matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6313a147",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChemDataset(Dataset):\n",
    "    def __init__(self, smiles, labels, precompute=False):\n",
    "        super(ChemDataset, self).__init__()\n",
    "        self.smiles = smiles\n",
    "        self.labels = labels\n",
    "        self.cache = {}\n",
    "\n",
    "        # Precomputing the dataset so the get method is faster, and the GPU doesn't have to wait for the CPU\n",
    "        if precompute:\n",
    "            print(f\"Precomputing the dataset in parellel...\")\n",
    "            with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "                futures = [\n",
    "                    executor.submit(self.process_key , idx)\n",
    "                    for idx in range(len(self.smiles))\n",
    "                ]\n",
    "\n",
    "                for future in as_completed(futures):\n",
    "                    future.result()\n",
    "\n",
    "            print(f\"Precomputation finished. {len(self.cache)} molecules cached.\")\n",
    "\n",
    "    def process_key(self, key):\n",
    "        smiles = self.smiles[key]\n",
    "        if smiles in self.cache.keys():\n",
    "            mol = self.cache[smiles]\n",
    "        else:\n",
    "            molgraph = MolGraph(smiles)\n",
    "            mol = self.molgraph2data(molgraph, key)\n",
    "            self.cache[smiles] = mol\n",
    "        return mol\n",
    "\n",
    "    def molgraph2data(self, molgraph, key):\n",
    "        data = tg.data.Data()\n",
    "\n",
    "        # Coverting all features and labels to tensors\n",
    "        # And adding it to the data object\n",
    "        data.x = torch.tensor(molgraph.f_atoms, dtype=torch.float)\n",
    "        data.edge_index = torch.tensor(molgraph.edge_index, dtype=torch.long).t().contiguous()\n",
    "        data.edge_attr = torch.tensor(molgraph.f_bonds, dtype=torch.float)\n",
    "        data.y = torch.tensor([self.labels[key]], dtype=torch.float)\n",
    "\n",
    "        data.smiles = self.smiles[key]\n",
    "\n",
    "        return data\n",
    "\n",
    "    def get(self,key):\n",
    "        return self.process_key(key)\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        return self.process_key(key)\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.smiles)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65270799",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_loader(data_df, smi_column, target_column, shuffle=True, batch_size=64):    \n",
    "    smiles = data_df[smi_column].values\n",
    "    labels = data_df[target_column].values.astype(np.float32)  \n",
    "    \n",
    "    dataset = ChemDataset(smiles, labels, precompute=True)\n",
    "    loader = DataLoader(dataset=dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=shuffle,\n",
    "                            pin_memory=True\n",
    "                       )\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfaef5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DMPNNConv(MessagePassing): # Extending the MessagePassing class from PyG\n",
    "    def __init__(self, hidden_size):\n",
    "        super(DMPNNConv, self).__init__(aggr='add') # Sum aggregation function\n",
    "        self.lin = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, edge_index, edge_attr):\n",
    "        row, _ = edge_index\n",
    "        # Since each edge is bidirectional, we do two message passings, one for each direction\n",
    "        aggregated_message = self.propagate(edge_index, x=None, edge_attr=edge_attr)\n",
    "        reversed_message = torch.flip(edge_attr.view(edge_attr.size(0) // 2, 2, -1), dims=[1]).view(edge_attr.size(0), -1)\n",
    "\n",
    "        return aggregated_message, self.lin(aggregated_message[row] - reversed_message)\n",
    "\n",
    "    def message(self, edge_attr):\n",
    "        return edge_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34df0c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(nn.Module):\n",
    "    def __init__(self, num_node_features, num_edge_features):\n",
    "        super(GNN, self).__init__()\n",
    "\n",
    "        self.depth = 3\n",
    "        self.hidden_size = 300\n",
    "        self.dropout = 0.02\n",
    "\n",
    "        # Encoder\n",
    "        self.edge_init = nn.Linear(num_node_features + num_edge_features, self.hidden_size)\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "\n",
    "        for _ in range(self.depth):\n",
    "            self.convs.append(DMPNNConv(self.hidden_size))\n",
    "            \n",
    "        self.edge_to_node = nn.Linear(num_node_features + self.hidden_size, self.hidden_size)\n",
    "        self.pool = global_add_pool\n",
    "\n",
    "        # Prediction head\n",
    "        self.ffn1 = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.ffn2 = nn.Linear(self.hidden_size, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "\n",
    "        # initial edge features\n",
    "        row, _ = edge_index\n",
    "        h_0 = F.relu(self.edge_init(torch.cat([x[row], edge_attr], dim=1)))\n",
    "        h = h_0\n",
    "\n",
    "        # convolutions\n",
    "        for l in range(self.depth):\n",
    "            _, h = self.convs[l](edge_index, h)\n",
    "            h += h_0\n",
    "            h = F.dropout(F.relu(h), self.dropout, training=self.training)\n",
    "\n",
    "        # dmpnn edge -> node aggregation\n",
    "        s, _ = self.convs[l](edge_index, h) #only use for summing\n",
    "        try:\n",
    "            q  = torch.cat([x,s], dim=1)\n",
    "        except:\n",
    "            print(data)\n",
    "            print(data.smiles)\n",
    "            print(data.x)\n",
    "            q  = torch.cat([x,s], dim=1)\n",
    "        h = F.relu(self.edge_to_node(q))\n",
    "\n",
    "        return self.ffn2(F.dropout(F.relu(self.ffn1(self.pool(h, batch))))).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd052185",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Standardizer:\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, x, rev=False):\n",
    "        if rev:\n",
    "            return (x * self.std) + self.mean\n",
    "        return (x - self.mean) / self.std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62ea2a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83402641",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, loss, stdzer):\n",
    "    model.train()\n",
    "    loss_all = 0\n",
    "\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out = model(data)\n",
    "        result = loss(out, stdzer(data.y))\n",
    "        result.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        loss_all += loss(stdzer(out, rev=True), data.y)\n",
    "\n",
    "    return math.sqrt(loss_all / len(loader.dataset))\n",
    "\n",
    "\n",
    "def pred(model, loader, loss, stdzer):\n",
    "    model.eval()\n",
    "\n",
    "    preds, ys = [], []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            data = data.to(device)\n",
    "            out = model(data)\n",
    "            pred = stdzer(out, rev=True)\n",
    "            preds.extend(pred.cpu().detach().tolist())\n",
    "\n",
    "    return preds\n",
    "\n",
    "\n",
    "def train():\n",
    "    torch.manual_seed(0)\n",
    "    data_df = pd.read_csv(\"AqSolDBc.csv\")\n",
    "    #Drop single atoms    \n",
    "    idx_single = [i for i,s in enumerate(data_df['SmilesCurated']) if Chem.MolFromSmiles(s).GetNumAtoms()==1 or '.' in s]\n",
    "    data_df = data_df.drop(idx_single)\n",
    "    if len(idx_single)>0:\n",
    "        print(f\"Removing {idx_single} due to single atoms\")\n",
    "\n",
    "        \n",
    "    test_df = pd.read_csv(\"OChemUnseen.csv\")\n",
    "    #Drop single atoms\n",
    "    idx_nonetype = [i for i,s in enumerate(test_df['SMILES']) if Chem.MolFromSmiles(s) is None] # Got an error for a SMILES which was None\n",
    "    test_df = test_df.drop(idx_nonetype)\n",
    "    if len(idx_nonetype)>0:\n",
    "        print(f\"Removing {idx_nonetype} due to Nonetypes\")\n",
    "\n",
    "    idx_single = [i for i,s in enumerate(test_df['SMILES']) if Chem.MolFromSmiles(s).GetNumAtoms()==1 or '.' in s]\n",
    "    test_df = test_df.drop(idx_single)\n",
    "    if len(idx_single)>0:\n",
    "        print(f\"Removing {idx_single} due to single atoms\")\n",
    "\n",
    "        \n",
    "    train_df, val_df = train_test_split(data_df, test_size=0.2, random_state=0)\n",
    "    train_loader = construct_loader(train_df, 'SmilesCurated', 'ExperimentalLogS', shuffle=True)\n",
    "    val_loader = construct_loader(val_df, 'SmilesCurated', 'ExperimentalLogS', shuffle=False)\n",
    "    test_loader = construct_loader(test_df, 'SMILES', 'LogS', shuffle=False)\n",
    "    mean = np.mean(train_loader.dataset.labels)\n",
    "    std = np.std(train_loader.dataset.labels)\n",
    "    stdzer = Standardizer(mean, std)\n",
    "\n",
    "    model = GNN(train_loader.dataset.num_node_features, train_loader.dataset.num_edge_features).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    loss = nn.MSELoss(reduction='sum').to(device)\n",
    "    print(model)\n",
    "    best_model = model\n",
    "    best_val_loss = 1000000\n",
    "    for epoch in range(0, 30):\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, loss, stdzer)\n",
    "        preds = pred(model, val_loader, loss, stdzer)\n",
    "        val_loss = root_mean_squared_error(preds,val_loader.dataset.labels)\n",
    "        print(\"Epoch\",epoch,\"  Train RMSE\", train_loss,\"   Val RMSE\", val_loss)\n",
    "        if val_loss < best_val_loss:\n",
    "            best_model = deepcopy(model)\n",
    "            best_val_loss = val_loss\n",
    "\n",
    "    preds = pred(best_model, test_loader, loss, stdzer)\n",
    "    print(\"Test RMSE\", root_mean_squared_error(preds,test_loader.dataset.labels))\n",
    "    print(\"Test MAE\", mean_absolute_error(preds,test_loader.dataset.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a5c5d5-2466-4aa0-87c9-1074170fb57f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing [1263, 1444, 3605, 3702] due to single atoms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[14:24:10] Explicit valence for atom # 1 P, 6, is greater than permitted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing [667] due to Nonetypes\n",
      "Removing [471, 503, 589, 591, 592, 593, 594, 610, 613, 641, 643, 647, 649, 652, 653, 654, 656, 658, 676, 681, 693, 744, 759, 763, 769, 773, 777, 807, 809, 811, 813, 869, 902, 969, 998] due to single atoms\n",
      "Precomputing the dataset in parellel...\n",
      "Precomputation finished. 6434 molecules cached.\n",
      "Precomputing the dataset in parellel...\n",
      "Precomputation finished. 1609 molecules cached.\n",
      "Precomputing the dataset in parellel...\n",
      "Precomputation finished. 2215 molecules cached.\n",
      "GNN(\n",
      "  (edge_init): Linear(in_features=51, out_features=300, bias=True)\n",
      "  (convs): ModuleList(\n",
      "    (0-2): 3 x DMPNNConv()\n",
      "  )\n",
      "  (edge_to_node): Linear(in_features=344, out_features=300, bias=True)\n",
      "  (ffn1): Linear(in_features=300, out_features=300, bias=True)\n",
      "  (ffn2): Linear(in_features=300, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
